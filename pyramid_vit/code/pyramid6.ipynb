{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2bPw-5W4hJe"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lb3YQA5HFeNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9bb0ce-455e-4a03-e31a-d0e9d15e27e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uyr3kLf61IIP"
      },
      "outputs": [],
      "source": [
        "global_var = {\n",
        "    # Resolutions\n",
        "    'RGB_img_res': (3, 192, 256),\n",
        "    'D_img_res': (1, 48, 64),\n",
        "    # Operations\n",
        "    'do_prints': True,\n",
        "    'do_print_model': True,\n",
        "    'do_pretrained': True,\n",
        "    'do_train': False,\n",
        "    'do_print_best_worst': True,\n",
        "    # ImageNet Initialization\n",
        "    'imagenet_w_init': False,\n",
        "    'imagenet_enc': 'METER_nyu_encoder_aug_mean0_var1_59', # A voi non interessa\n",
        "    # Parameters\n",
        "    'dts_type': 'nyu',\n",
        "    'architecture_type': 's',\n",
        "    'seed': 10000,\n",
        "    'lr': 1e-3,\n",
        "    'lr_patience': 15,\n",
        "    'epochs': 120, # Scegliete VOI,\n",
        "    'batch_size': 64,\n",
        "    'batch_size_eval': 1,\n",
        "    'n_workers': 2,\n",
        "    'e_stop_epochs': 30,\n",
        "    'size_train': None,\n",
        "    'size_test': None,\n",
        "}\n",
        "\n",
        "augmentation_parameters = {\n",
        "    'flip': 0.5,\n",
        "    'mirror': 0.5,\n",
        "    'color&bright': 0.5,\n",
        "    'c_swap': 0.5,\n",
        "    'random_crop': 0.5,\n",
        "    'random_d_shift': 0.5  # range(+-10)cm\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KAAYJlZW4p9a"
      },
      "outputs": [],
      "source": [
        "# Paths ## DA CONTROLLARE CON DRIVE \"/content/drive/MyDrive/...\"\n",
        "dataset_root = '/content/drive/MyDrive/Visope_project/'\n",
        "save_model_root = '/content/drive/MyDrive/Visope_project/pyramid/results/'\n",
        "imagenet_init = '/work/imagenet/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb6YUgKM5H_i"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dVq5yvth7LJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f224ed8f-a166-4387-e6d0-4c7c475d49bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchsummaryX) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchsummaryX) (16.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchsummaryX) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchsummaryX) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LOqnce-65LvH"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage.transform as st\n",
        "import torch\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummaryX import summary\n",
        "from scipy.interpolate import LinearNDInterpolator\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import random\n",
        "import torchvision.transforms as TT\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "from itertools import product\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import exp\n",
        "from einops import rearrange\n",
        "import csv\n",
        "import math\n",
        "from time import perf_counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network_type = \"pyramidalMETER\"\n",
        "old_stout = sys.stdout"
      ],
      "metadata": {
        "id": "nH7MtTLD-ER4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2TAq6B5UBI"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s77jAM-X5VsY"
      },
      "outputs": [],
      "source": [
        "def hardware_check():\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Actual device: \", device)\n",
        "    if 'cuda' in device:\n",
        "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
        "\n",
        "    return device\n",
        "\n",
        "\n",
        "def plot_depth_map(dm):\n",
        "\n",
        "    MIN_DEPTH = 0.0\n",
        "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
        "\n",
        "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
        "    cmap = plt.cm.plasma_r\n",
        "\n",
        "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
        "\n",
        "\n",
        "def resize_keeping_aspect_ratio(img, base):\n",
        "    \"\"\"\n",
        "    Resize the image to a defined length manteining its proportions\n",
        "    Scaling the shortest side of the image to a fixed 'base' length'\n",
        "    \"\"\"\n",
        "\n",
        "    if img.shape[0] <= img.shape[1]:\n",
        "        basewidth = int(base)\n",
        "        wpercent = (basewidth / float(img.shape[0]))\n",
        "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
        "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
        "    else:\n",
        "        baseheight = int(base)\n",
        "        wpercent = (baseheight / float(img.shape[1]))\n",
        "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
        "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def compute_rmse(predictions, depths):\n",
        "    valid_mask = depths > 0.0\n",
        "    valid_predictions = predictions[valid_mask]\n",
        "    valid_depths = depths[valid_mask]\n",
        "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
        "    return torch.sqrt(mse)\n",
        "\n",
        "\n",
        "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
        "    valid_mask = y_true > 0.0\n",
        "    valid_pred = y_pred[valid_mask]\n",
        "    valid_true = y_true[valid_mask]\n",
        "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
        "    return 100 * torch.mean(correct.float())\n",
        "\n",
        "\n",
        "def print_model(model, device, save_model_root, input_shape):\n",
        "    info = summary(model, torch.ones((1, input_shape[0], input_shape[1], input_shape[2])).to(device))\n",
        "    info.to_csv(save_model_root + 'model_summary.csv')\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "def save_checkpoint(model, name, path_save_model):\n",
        "    \"\"\"\n",
        "    Saves a model\n",
        "    \"\"\"\n",
        "    if '_best' in name:\n",
        "        folder = name.split(\"_best\")[0]\n",
        "    elif '_checkpoint' in name:\n",
        "        folder = name.split(\"_checkpoint\")[0]\n",
        "    if not os.path.isdir(path_save_model):\n",
        "        os.makedirs(path_save_model, exist_ok=True)\n",
        "    torch.save(model.state_dict(), path_save_model + name)\n",
        "\n",
        "\n",
        "def save_history(history, filepath):\n",
        "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
        "    pickle.dump(history, tmp_file)\n",
        "    tmp_file.close()\n",
        "\n",
        "\n",
        "def save_csv_history(model_name, path):\n",
        "    objects = []\n",
        "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    df = pd.DataFrame(objects)\n",
        "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
        "\n",
        "\n",
        "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
        "    model_name = model.__class__.__name__\n",
        "\n",
        "    if do_pretrained:\n",
        "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
        "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
        "        model.load_state_dict(model_dict)\n",
        "        print(\"checkpoint loaded\\n\")\n",
        "\n",
        "    if imagenet_w_init:\n",
        "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
        "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
        "        model_dict = model.state_dict()\n",
        "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
        "\n",
        "        # pretrained_param = len(pretrained_dict.items())\n",
        "        counter_param = 0\n",
        "        for i, j in pretrained_dict.items():\n",
        "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
        "                counter_param += 1\n",
        "\n",
        "        print(f'Pertained parameters: {counter_param}\\n')\n",
        "\n",
        "        # 1. filter out unnecessary keys\n",
        "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
        "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
        "        # 2. overwrite entries in the existing state dict\n",
        "        model_dict.update(pretrained_dict)\n",
        "        # 3. load the new state dict\n",
        "        model.load_state_dict(model_dict)\n",
        "\n",
        "        # alternativa to 2 e 3\n",
        "        # model.load_state_dict(pretrained_dict, strict=False)\n",
        "        print(\"Partial initialization computed\\n\")\n",
        "\n",
        "    return model, model_name\n",
        "\n",
        "\n",
        "def plot_graph(f, g, f_label, g_label, title, path):\n",
        "    epochs = range(0, len(f))\n",
        "    plt.plot(epochs, f, 'b', label=f_label)\n",
        "    plt.plot(epochs, g, 'orange', label=g_label)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid('on', color='#cfcfcf')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + title + '.pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_history(history, path):\n",
        "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
        "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
        "\n",
        "\n",
        "def plot_loss_parts(history, path, title):\n",
        "    l_mae_list = history['l_mae']\n",
        "    l_norm_list = history['l_norm']\n",
        "    l_grad_list = history['l_grad']\n",
        "    l_ssim_list = history['l_ssim']\n",
        "    epochs = range(0, len(l_mae_list))\n",
        "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
        "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
        "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
        "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.grid('on', color='#cfcfcf')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + title + '.pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
        "    for i in range(quantity):\n",
        "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
        "\n",
        "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
        "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
        "              f' variance =  {torch.var(img)}\\n')\n",
        "\n",
        "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title('Input image')\n",
        "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "        if not False:\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title('Grayscale DepthMap')\n",
        "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        if not False:\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title('Colored DepthMap')\n",
        "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
        "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        if not False:\n",
        "            plt.axis('off')\n",
        "\n",
        "        print(\"************************** \",save_model_root)\n",
        "        save_path = save_model_root + 'example&augment_img/'\n",
        "        print(\"************************** \",save_path)\n",
        "        if not os.path.exists(save_path):\n",
        "            os.mkdir(save_path)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
        "        plt.close(fig=fig)\n",
        "\n",
        "\n",
        "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
        "    \"\"\"\n",
        "    Shows prediction example\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
        "    for i, index in zip(range(len(indices)), indices):\n",
        "        img, depth = dataset.__getitem__(index)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        # Predict\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(torch.from_numpy(img).to(device))\n",
        "            # Build plot\n",
        "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
        "            plt.subplot(1, len(indices), i+1)\n",
        "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
        "            cbar = plt.colorbar()\n",
        "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
        "            if False:\n",
        "                plt.axis('off')\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
        "    plt.close(fig=fig)\n",
        "\n",
        "\n",
        "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
        "    save_path = save_model_root + type + '_predictions/'\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for i in range(len(list_type)):\n",
        "        index_image = list_type[i][0]\n",
        "        rmse_value = list_type[i][1]\n",
        "\n",
        "        img, depth = dataset.__getitem__(index=index_image)\n",
        "\n",
        "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.title(f'Original image {index_image}')\n",
        "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.title('Ground Truth')\n",
        "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
        "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predict\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
        "\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.title('Predicted DepthMap')\n",
        "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
        "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
        "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
        "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
        "        plt.close(fig=fig)\n",
        "\n",
        "\n",
        "def compute_MeanVar(dataset):\n",
        "    r_mean, g_mean, b_mean = [], [], []\n",
        "    r_var, g_var, b_var = [], [], []\n",
        "    for i in range(dataset.__len__()):\n",
        "        img, _ = dataset.__getitem__(index=i)\n",
        "        r = np.array(img[0, :, :])\n",
        "        g = np.array(img[1, :, :])\n",
        "        b = np.array(img[2, :, :])\n",
        "\n",
        "        r_mean.append(np.mean(r))\n",
        "        g_mean.append(np.mean(g))\n",
        "        b_mean.append(np.mean(b))\n",
        "\n",
        "        r_var.append(np.var(r))\n",
        "        g_var.append(np.var(g))\n",
        "        b_var.append(np.var(b))\n",
        "\n",
        "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
        "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
        "\n",
        "\n",
        "def compute_MeanImg(dataset, save_model_root):\n",
        "    r, g, b = [], [], []\n",
        "    for i in range(dataset.__len__()):\n",
        "        img, _ = dataset.__getitem__(index=i)\n",
        "        r.append(np.array(img[0, :, :]))\n",
        "        g.append(np.array(img[1, :, :]))\n",
        "        b.append(np.array(img[2, :, :]))\n",
        "\n",
        "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
        "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
        "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
        "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
        "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
        "\n",
        "    print(\"Process Completed\")\n",
        "\n",
        "def blockPrint():\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "# Restore\n",
        "def enablePrint():\n",
        "    sys.stdout = old_stout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQYEe4V5j5E"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zUH-3na7eAH"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0CFEYqVd7i1H"
      },
      "outputs": [],
      "source": [
        "def pixel_shift(depth_img, shift):\n",
        "    depth_img = depth_img + shift\n",
        "    return depth_img\n",
        "\n",
        "\n",
        "def random_crop(x, y, crop_size=(192, 256)):\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    assert x.shape[1] == y.shape[1]\n",
        "    h, w, _ = x.shape\n",
        "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
        "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
        "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
        "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
        "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
        "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
        "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
        "    if cropped_y.shape[-1] == 0:\n",
        "        return x, y\n",
        "    else:\n",
        "        return cropped_x, cropped_y\n",
        "\n",
        "\n",
        "def augmentation2D(img, depth, print_info_aug):\n",
        "    # Random flipping\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
        "        img = (img[..., ::1, :, :]).copy()\n",
        "        depth = (depth[..., ::1, :, :]).copy()\n",
        "        if print_info_aug:\n",
        "            print('--> Random flipped')\n",
        "    # Random mirroring\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
        "        img = (img[..., ::-1, :]).copy()\n",
        "        depth = (depth[..., ::-1, :]).copy()\n",
        "        if print_info_aug:\n",
        "            print('--> Random mirrored')\n",
        "    # Augment image\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
        "        # gamma augmentation\n",
        "        gamma = random.uniform(0.9, 1.1)\n",
        "        img = img ** gamma\n",
        "        brightness = random.uniform(0.9, 1.1)\n",
        "        img = img * brightness\n",
        "        # color augmentation\n",
        "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
        "        white = np.ones((img.shape[0], img.shape[1]))\n",
        "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
        "        img *= color_image\n",
        "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
        "        if print_info_aug:\n",
        "            print('--> Image randomly augmented')\n",
        "    # Channel swap\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
        "        indices = list(product([0, 1, 2], repeat=3))\n",
        "        policy_idx = random.randint(0, len(indices) - 1)\n",
        "        img = img[..., list(indices[policy_idx])]\n",
        "        if print_info_aug:\n",
        "            print('--> Channel swapped')\n",
        "    # Random crop\n",
        "    if random.random() <= augmentation_parameters['random_crop']:\n",
        "        img, depth = random_crop(img, depth)\n",
        "        if print_info_aug:\n",
        "            print('--> Random cropped')\n",
        "    # Depth Shift\n",
        "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
        "        random_shift = random.randint(-10, 10)\n",
        "        depth = pixel_shift(depth, shift=random_shift)\n",
        "        if print_info_aug:\n",
        "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
        "\n",
        "    return img, depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbCnAwv453IN"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zhuQQhwf597g"
      },
      "outputs": [],
      "source": [
        "class NYU2_Dataset:\n",
        "    \"\"\"\n",
        "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
        "      * 654 Test and 50688 Train images\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
        "        self.dataset = path\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.info = 0\n",
        "        self.dts_type = dts_type\n",
        "        self.aug = aug\n",
        "        self.rgb_h_res = rgb_h_res\n",
        "        self.d_h_res = d_h_res\n",
        "        self.scenarios = scenarios\n",
        "\n",
        "        # Handle dataset\n",
        "        if self.dts_type == 'test':\n",
        "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
        "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
        "\n",
        "            rgb = np.load(img_path)\n",
        "            depth = np.load(depth_path)\n",
        "\n",
        "            self.x = rgb\n",
        "            self.y = depth\n",
        "\n",
        "            if dts_size != 0:\n",
        "                self.x = rgb[:dts_size]\n",
        "                self.y = depth[:dts_size]\n",
        "\n",
        "            self.info = len(self.x)\n",
        "\n",
        "        elif self.dts_type == 'train':\n",
        "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
        "            for scene in scenarios:\n",
        "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
        "                for el in elem:\n",
        "                    if 'jpg' in el:\n",
        "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
        "                    elif 'png' in el:\n",
        "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
        "                    else:\n",
        "                        raise SystemError('Type image error (train)')\n",
        "\n",
        "            if len(self.x) != len(self.y):\n",
        "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
        "\n",
        "            self.x.sort()\n",
        "            self.y.sort()\n",
        "\n",
        "            if dts_size != 0:\n",
        "                self.x = self.x[:dts_size]\n",
        "                self.y = self.y[:dts_size]\n",
        "\n",
        "            self.info = len(self.x)\n",
        "\n",
        "        else:\n",
        "            raise SystemError('Problem in the path')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.info\n",
        "\n",
        "    def __getitem__(self, index=None, print_info_aug=False):\n",
        "        if index is None:\n",
        "            index = np.random.randint(0, self.info)\n",
        "\n",
        "        # Load Image\n",
        "        if self.dts_type == 'test':\n",
        "            img = self.x[index]\n",
        "        else:\n",
        "            img = Image.open(self.dataset + self.x[index]).convert('RGB')\n",
        "            img = np.array(img)\n",
        "\n",
        "        # Load Depth Image\n",
        "        if self.dts_type == 'test':\n",
        "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
        "        else:\n",
        "            depth = Image.open(self.dataset + self.y[index])\n",
        "            depth = np.array(depth) / 255\n",
        "            depth = np.clip(depth * 1000, 50, 1000)\n",
        "            depth = np.expand_dims(depth, axis=-1)\n",
        "\n",
        "        # Augmentation\n",
        "        if self.aug:\n",
        "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
        "\n",
        "        img_post_processing = TT.Compose([\n",
        "            TT.ToTensor(),\n",
        "            TT.Resize((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), antialias=True),\n",
        "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
        "        ])\n",
        "        depth_post_processing = TT.Compose([\n",
        "            TT.ToTensor(),\n",
        "            TT.Resize((global_var['D_img_res'][1], global_var['D_img_res'][2]), antialias=True),\n",
        "        ])\n",
        "\n",
        "        img = img_post_processing(img/255)\n",
        "        depth = depth_post_processing(depth)\n",
        "\n",
        "        return img.float(), depth.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLUvGTJF55VD"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KKUmyNBL5m1o"
      },
      "outputs": [],
      "source": [
        "def init_train_test_loader(dts_type, dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
        "    if dts_type == 'nyu':\n",
        "        Dataset_class = NYU2_Dataset\n",
        "        dts_root_path = dts_root_path + 'NYUv2/'\n",
        "    else:\n",
        "        print('OCCHIO AL DATASET')\n",
        "\n",
        "\n",
        "    # Load Datasets\n",
        "    test_Dataset = Dataset_class(\n",
        "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
        "    )\n",
        "    training_Dataset = Dataset_class(\n",
        "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
        "    )\n",
        "    # Create Dataloaders\n",
        "    training_DataLoader = DataLoader(\n",
        "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
        "    )\n",
        "    test_DataLoader = DataLoader(\n",
        "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAWQQQzf8ctn"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DTaUymYS8mDJ"
      },
      "outputs": [],
      "source": [
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
        "    L = val_range\n",
        "\n",
        "    padd = 0\n",
        "    (_, channel, height, width) = img1.size()\n",
        "    if window is None:\n",
        "        real_size = min(window_size, height, width)\n",
        "        window = create_window(real_size, channel=channel).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * L) ** 2\n",
        "    C2 = (0.03 * L) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "    if size_average:\n",
        "        ret = ssim_map.mean()\n",
        "    else:\n",
        "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "    if full:\n",
        "        return ret, cs\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "class Sobel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sobel, self).__init__()\n",
        "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
        "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
        "        edge_k = np.stack((edge_kx, edge_ky))\n",
        "\n",
        "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
        "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.edge_conv(x)\n",
        "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class balanced_loss_function(nn.Module):\n",
        "\n",
        "    def __init__(self, device):\n",
        "        super(balanced_loss_function, self).__init__()\n",
        "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
        "        self.get_gradient = Sobel().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, output, depth):\n",
        "        with torch.no_grad():\n",
        "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
        "\n",
        "        depth_grad = self.get_gradient(depth)\n",
        "        output_grad = self.get_gradient(output)\n",
        "\n",
        "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
        "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
        "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
        "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
        "\n",
        "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
        "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
        "\n",
        "        loss_depth = torch.abs(output - depth).mean()\n",
        "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
        "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
        "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
        "\n",
        "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
        "\n",
        "        loss_grad = (loss_dx + loss_dy) / 2\n",
        "\n",
        "        return loss_depth, loss_ssim, loss_normal, loss_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhLCflR28fxo"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M9_rdzyp87dB"
      },
      "outputs": [],
      "source": [
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU()  # nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   groups=depth,\n",
        "                                   padding=1,\n",
        "                                   stride=stride,\n",
        "                                   bias=bias).to(device)\n",
        "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
        "    return nn.Sequential(\n",
        "        # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
        "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
        "                        bias=False, device='cpu'),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU()  # nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),  # nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        # head_dim = dim // heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        # print(\"------------------------------------DIM--------------------------\", dim)\n",
        "        self.q = nn.Linear(dim, dim, bias=False)\n",
        "        self.kv = nn.Linear(dim, dim * 2, bias=False)\n",
        "        self.attn_drop = nn.Dropout(0)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(0)\n",
        "\n",
        "        self.sr_ratio = 4\n",
        "        if self.sr_ratio > 1:\n",
        "            self.sr = nn.Conv2d(dim, dim, kernel_size=self.sr_ratio, stride=self.sr_ratio)\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C, W = x.shape # torch.Size([1, 4, 192, 144])\n",
        "\n",
        "        q = self.q(x)\n",
        "\n",
        "        if self.sr_ratio > 1:\n",
        "            # x_ = x.permute(0, 2, 1).reshape(B, C, N, W)\n",
        "            x_ = x.reshape(B, W, N, C)\n",
        "            # print(\"-------------------------------------------------------------------\", x_.size)\n",
        "            x_ = self.sr(x_).reshape(B, -1, self.dim).permute(0, 2, 1)\n",
        "            # print(\"-------------------------------------------------------------------\", x_.size)\n",
        "            x_ = self.norm(x_.permute(0, 2, 1))\n",
        "            kv = self.kv(x_).reshape(B, -1, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
        "        else:\n",
        "            kv = self.kv(x).reshape(B, -1, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        q = q.reshape([q.shape[0], q.shape[1], q.shape[2]*(q.shape[3]//k.shape[3]), k.shape[3]])\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C, W)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(inp * expansion)\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU(),  # nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU(),  # nn.SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU(),  # nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)  # Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
        "\n",
        "        self.conv3 = conv_1x1_bn(dim, channel)\n",
        "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        #print(\"*********************************** Start logging ***********************************\")\n",
        "        #print(\"Transformer input shape: \",x.shape)\n",
        "\n",
        "        # Global representations\n",
        "        _, _, h, w = x.shape\n",
        "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
        "        #print(\"Rearranged input shape: \",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        #print(\"Transformer output shape: \",x.shape)\n",
        "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n",
        "                      pw=self.pw)\n",
        "        #print(\"Rearranged output shape: \",x.shape)\n",
        "        #print(\"**************************************************************************************\")\n",
        "        # Fusion\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat((x, y), 1)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViT(nn.Module):\n",
        "    def __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        ph, pw = patch_size\n",
        "        assert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "        L = [1, 1, 1]  # L = [2, 4, 3] # --> +5 FPS\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
        "\n",
        "        self.mv2 = nn.ModuleList([])\n",
        "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
        "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "\n",
        "        self.mvit = nn.ModuleList([])\n",
        "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
        "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
        "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
        "\n",
        "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "        # self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        # self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y0 = self.conv1(x)\n",
        "        x = self.mv2[0](y0)\n",
        "\n",
        "        y1 = self.mv2[1](x)\n",
        "        x = self.mv2[2](y1)\n",
        "        x = self.mv2[3](x)  # Repeat\n",
        "\n",
        "        y2 = self.mv2[4](x)\n",
        "        x = self.mvit[0](y2)\n",
        "\n",
        "        y3 = self.mv2[5](x)\n",
        "        x = self.mvit[1](y3)\n",
        "\n",
        "        x = self.mv2[6](x)\n",
        "        x = self.mvit[2](x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return x, [y0, y1, y2, y3]\n",
        "\n",
        "\n",
        "def mobilevit_xxs():\n",
        "    enc_type = 'xxs'\n",
        "    dims = [64, 80, 96]\n",
        "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]  # 320\n",
        "    return MobileViT((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), dims, channels, num_classes=1000, expansion=2), enc_type\n",
        "\n",
        "\n",
        "def mobilevit_xs():\n",
        "    enc_type = 'xs'\n",
        "    dims = [96, 120, 144]\n",
        "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 192] # 384\n",
        "    return MobileViT((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), dims, channels, num_classes=1000), enc_type\n",
        "\n",
        "\n",
        "def mobilevit_s():\n",
        "    enc_type = 's'\n",
        "    dims = [144, 192, 240]\n",
        "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
        "    return MobileViT((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), dims, channels, num_classes=1000), enc_type\n",
        "\n",
        "\n",
        "class UpSample_layer(nn.Module):\n",
        "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
        "        super(UpSample_layer, self).__init__()\n",
        "        self.flag = flag\n",
        "        self.name = name\n",
        "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
        "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
        "        self.end_up_layer = nn.Sequential(\n",
        "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_layer):\n",
        "        x = self.conv2d_transpose(x)\n",
        "        if x.shape[-1] != enc_layer.shape[-1]:\n",
        "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n",
        "        if x.shape[-1] != enc_layer.shape[-1]:\n",
        "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n",
        "        x = torch.cat([x, enc_layer], dim=1)\n",
        "        x = self.end_up_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SPEED_decoder(nn.Module):\n",
        "    def __init__(self, device, typ):\n",
        "        super(SPEED_decoder, self).__init__()\n",
        "        self.conv2d_in = nn.Conv2d(320 if typ == 's' else 192 if typ == 'xs' else 160,\n",
        "                                   128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
        "                                   kernel_size=(1, 1), padding='same', bias=False)\n",
        "        self.ups_block_1 = UpSample_layer(128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
        "                                          64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
        "                                          flag=True,\n",
        "                                          sep_conv_filters=192 if typ == 's' else 144 if typ == 'xs' else 96,\n",
        "                                          name='up1', device=device)\n",
        "        self.ups_block_2 = UpSample_layer(64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
        "                                          32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
        "                                          flag=False,\n",
        "                                          sep_conv_filters=128 if typ == 's' else 96 if typ == 'xs' else 64,\n",
        "                                          name='up2', device=device)\n",
        "        self.ups_block_3 = UpSample_layer(32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
        "                                          16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
        "                                          flag=False,\n",
        "                                          sep_conv_filters=80 if typ == 's' else 64 if typ == 'xs' else 32,\n",
        "                                          name='up3', device=device)\n",
        "        self.conv2d_out = nn.Conv2d(16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
        "                                    1, kernel_size=(3, 3), padding='same', bias=False)\n",
        "\n",
        "    def forward(self, x, enc_layer_list):\n",
        "        x = self.conv2d_in(x)\n",
        "        x = self.ups_block_1(x, enc_layer_list[3])\n",
        "        x = self.ups_block_2(x, enc_layer_list[2])\n",
        "        x = self.ups_block_3(x, enc_layer_list[1])\n",
        "        x = self.conv2d_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class build_model(nn.Module):\n",
        "    \"\"\"\n",
        "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, device, arch_type):\n",
        "        super(build_model, self).__init__()\n",
        "        if arch_type == 's':\n",
        "            self.encoder, enc_type = mobilevit_s()\n",
        "        elif arch_type == 'xs':\n",
        "            self.encoder, enc_type = mobilevit_xs()\n",
        "        else:\n",
        "            self.encoder, enc_type = mobilevit_xxs()\n",
        "        self.decoder = SPEED_decoder(device=device, typ=enc_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, enc_layer = self.encoder(x)\n",
        "        x = self.decoder(x, enc_layer)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAvmPzvA8Pu-"
      },
      "source": [
        "# Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "es_aqA008Sof"
      },
      "outputs": [],
      "source": [
        "def log10(x):\n",
        "    return torch.log(x) / math.log(10)\n",
        "\n",
        "\n",
        "class Result(object):\n",
        "    def __init__(self):\n",
        "        self.irmse, self.imae = 0, 0\n",
        "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
        "        self.absrel, self.lg10 = 0, 0\n",
        "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
        "\n",
        "    def set_to_worst(self):\n",
        "        self.irmse, self.imae = np.inf, np.inf\n",
        "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
        "        self.absrel, self.lg10 = np.inf, np.inf\n",
        "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
        "\n",
        "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
        "        self.irmse, self.imae = irmse, imae\n",
        "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
        "        self.absrel, self.lg10 = absrel, lg10\n",
        "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
        "\n",
        "    def evaluate(self, output, target):\n",
        "        valid_mask = target > 0\n",
        "\n",
        "        output = output[valid_mask]\n",
        "        target = target[valid_mask]\n",
        "\n",
        "        if 'kitti' in global_var['dts_type']:\n",
        "            output = output[2080:] # remove first 13pixels lines\n",
        "            target = target[2080:]\n",
        "\n",
        "        abs_diff = (output - target).abs()\n",
        "\n",
        "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
        "        self.rmse = math.sqrt(self.mse)\n",
        "        self.mae = float(abs_diff.mean())\n",
        "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
        "        self.absrel = float((abs_diff / target).mean())\n",
        "\n",
        "        maxRatio = torch.max(output / target, target / output)\n",
        "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
        "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
        "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
        "\n",
        "        inv_output = 1 / output\n",
        "        inv_target = 1 / target\n",
        "        abs_inv_diff = (inv_output - inv_target).abs()\n",
        "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
        "        self.imae = float(abs_inv_diff.mean())\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.count = 0.0\n",
        "        self.sum_irmse, self.sum_imae = 0, 0\n",
        "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
        "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
        "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
        "\n",
        "    def update(self, result, n=1):\n",
        "        self.count += n\n",
        "\n",
        "        self.sum_irmse += n * result.irmse\n",
        "        self.sum_imae += n * result.imae\n",
        "        self.sum_mse += n * result.mse\n",
        "        self.sum_rmse += n * result.rmse\n",
        "        self.sum_mae += n * result.mae\n",
        "        self.sum_absrel += n * result.absrel\n",
        "        self.sum_lg10 += n * result.lg10\n",
        "        self.sum_delta1 += n * result.delta1\n",
        "        self.sum_delta2 += n * result.delta2\n",
        "        self.sum_delta3 += n * result.delta3\n",
        "\n",
        "    def average(self):\n",
        "        avg = Result()\n",
        "        avg.update(\n",
        "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
        "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
        "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
        "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
        "        return avg\n",
        "\n",
        "\n",
        "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
        "    best_worst_dict = {}\n",
        "    result = Result()\n",
        "    result.set_to_worst()\n",
        "    average_meter = AverageMeter()\n",
        "    model.eval()  # switch to evaluate mode\n",
        "\n",
        "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
        "        inputs, depths = inputs.cpu(), depths.cpu()\n",
        "        # compute output\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "        result.evaluate(predictions, depths)\n",
        "        average_meter.update(result)  # (result, inputs.size(0))\n",
        "        best_worst_dict[i] = result.rmse\n",
        "\n",
        "    avg = average_meter.average()\n",
        "\n",
        "    print('MAE={average.mae:.3f}\\n'\n",
        "          'RMSE={average.rmse:.3f}\\n'\n",
        "          'Delta1={average.delta1:.3f}\\n'\n",
        "          'REL={average.absrel:.3f}\\n'\n",
        "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
        "\n",
        "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
        "        writer.writeheader()\n",
        "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
        "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
        "\n",
        "    return best_worst_dict, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjiqGK4q42zP"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "590sGQdh45FS",
        "outputId": "5e5cdbab-651b-4a2e-b682-090e0dc53868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-ae44bf5dd008>\u001b[0m in \u001b[0;36m<cell line: 211>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time elapsed: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \"\"\"\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "def process(device):\n",
        "    # Set-seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "    np.random.seed(global_var['seed'])\n",
        "    torch.cuda.manual_seed(global_var['seed'])\n",
        "    # Datasets loading\n",
        "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
        "        dts_type=global_var['dts_type'],\n",
        "        dts_root_path=dataset_root,\n",
        "        rgb_h_res=global_var['RGB_img_res'][1],\n",
        "        d_h_res=global_var['D_img_res'][1],\n",
        "        bs_train=global_var['batch_size'],\n",
        "        bs_eval=global_var['batch_size_eval'],\n",
        "        num_workers=global_var['n_workers'],\n",
        "        size_train=global_var['size_train'],\n",
        "        size_test=global_var['size_test']\n",
        "    )\n",
        "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
        "    # Prints samples\n",
        "    if global_var['do_prints']:\n",
        "        print(' --- Test samples --- ')\n",
        "        print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
        "                  save_model_root=save_model_root)\n",
        "        print(' --- Training augmented samples --- ')\n",
        "        print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
        "                  save_model_root=save_model_root)\n",
        "    if global_var['do_train']:\n",
        "        torch.cuda.empty_cache()\n",
        "        # Globals\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
        "                   'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
        "        min_rmse = float('inf')\n",
        "        min_acc = 0\n",
        "        train_loss_list = []\n",
        "        test_loss_list = []\n",
        "        # Loss\n",
        "        criterion = balanced_loss_function(device=device)\n",
        "        # Model\n",
        "        model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
        "        if global_var['do_pretrained'] or global_var['imagenet_w_init']:\n",
        "            model, _ = load_pretrained_model(model=model,\n",
        "                                             path_weigths=save_model_root + 'build_model_best' if global_var['do_pretrained']\n",
        "                                                          else imagenet_init + global_var['imagenet_enc'] + '/build_model_best',\n",
        "                                             device=device,\n",
        "                                             do_pretrained=global_var['do_pretrained'],\n",
        "                                             imagenet_w_init=global_var['imagenet_w_init'])\n",
        "        model_name = model.__class__.__name__\n",
        "        if global_var['do_print_model']:\n",
        "            print_model(model=model, device=device, save_model_root=save_model_root, input_shape=global_var['RGB_img_res'])\n",
        "        print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "        # Optimizer\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(), lr=global_var['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
        "        )\n",
        "        # Scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.1, patience=global_var['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
        "            cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
        "        )\n",
        "        # Early stopping\n",
        "        trigger_times, early_stopping_epochs = 0, global_var['e_stop_epochs']\n",
        "        print(\"Start training: {}\\n\".format(model_name))\n",
        "        # Train\n",
        "        for epoch in range(global_var['epochs']):\n",
        "            iter = 1\n",
        "            model.train()\n",
        "            running_loss, accuracy = 0, 0\n",
        "            running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
        "            with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "                for batch in tepoch:\n",
        "                    tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Training\")\n",
        "                    # Load data\n",
        "                    inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
        "                    # Forward\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    # Compute loss\n",
        "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
        "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
        "                    # Backward\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    # Evaluation and Stats\n",
        "                    running_loss += loss.item()\n",
        "                    running_l_mae += loss_depth.item()\n",
        "                    running_l_norm += loss_normal.item()\n",
        "                    running_l_grad += loss_grad.item()\n",
        "                    running_l_ssim += loss_ssim.item()\n",
        "\n",
        "                    train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
        "                    train_loss_list.append(train_loss_support)\n",
        "\n",
        "                    accuracy += compute_accuracy(outputs, depths)\n",
        "                    tepoch.set_postfix({'Loss': running_loss / iter,\n",
        "                                        'Acc': accuracy.item() / iter,\n",
        "                                        'Lr': global_var['lr'] if not history['lrs'] else history['lrs'][-1],\n",
        "                                        'L_mae': running_l_mae / iter,\n",
        "                                        'L_norm': running_l_norm / iter,\n",
        "                                        'L_grad': running_l_grad / iter,\n",
        "                                        'L_ssim': running_l_ssim / iter\n",
        "                                        })\n",
        "                    iter += 1\n",
        "\n",
        "            # Validation\n",
        "            iter = 1\n",
        "            model.eval()\n",
        "            test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
        "            with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "                for batch in tepoch:\n",
        "                    tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Validation\")\n",
        "                    inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
        "                    # Validation loop\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(inputs)\n",
        "                        # Evaluation metrics\n",
        "                        test_accuracy += compute_accuracy(outputs, depths)\n",
        "                        # Loss\n",
        "                        loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
        "                        loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
        "                        test_loss += loss.item()\n",
        "\n",
        "                        test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
        "                        test_loss_list.append(test_loss_support)\n",
        "\n",
        "                        # RMSE\n",
        "                        test_rmse += compute_rmse(outputs, depths)\n",
        "                        tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
        "                                            'RMSE': test_rmse.item() / iter})\n",
        "                        iter += 1\n",
        "\n",
        "            # Update history infos\n",
        "            history['lrs'].append(get_lr(optimizer))\n",
        "            history['train_loss'].append(running_loss / len(training_DataLoader))\n",
        "            history['val_loss'].append(test_loss / len(test_DataLoader))\n",
        "            history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
        "            history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
        "            history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
        "            # Update history losses infos\n",
        "            history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
        "            history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
        "            history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
        "            history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
        "            # Update scheduler LR\n",
        "            scheduler.step(history['test_rmse'][-1])\n",
        "            # Save model by best RMSE\n",
        "            if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
        "                trigger_times = 0\n",
        "                min_rmse = test_rmse / len(test_DataLoader)\n",
        "                save_checkpoint(model, model_name + '_best', save_model_root)\n",
        "                print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
        "            else:\n",
        "                trigger_times += 1\n",
        "                print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
        "            # Save model by best ACCURACY\n",
        "            if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
        "                min_acc = test_accuracy / len(test_DataLoader)\n",
        "                save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
        "                print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
        "                if trigger_times > 4:\n",
        "                    trigger_times = trigger_times - 2\n",
        "                    print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
        "\n",
        "            save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
        "                                     save_path=save_model_root + 'evolution_img/')\n",
        "            save_history(history, save_model_root + model_name + '_history')\n",
        "            # Empty CUDA cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if trigger_times == early_stopping_epochs:\n",
        "                print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
        "                break\n",
        "\n",
        "            # Save loss for graphs\n",
        "            np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
        "            np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
        "\n",
        "        print('Finished Training')\n",
        "        save_csv_history(model_name=model_name, path=save_model_root)\n",
        "        plot_history(history, path=save_model_root)\n",
        "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
        "\n",
        "        if global_var['do_prints']:\n",
        "            if os.path.exists(save_model_root + 'example&augment_img/'):\n",
        "                shutil.rmtree(save_model_root + 'example&augment_img/')\n",
        "\n",
        "    else:\n",
        "        model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
        "        model, model_name = load_pretrained_model(model=model,\n",
        "                                                  path_weigths=save_model_root + 'build_model_best',\n",
        "                                                  device=device,\n",
        "                                                  do_pretrained=global_var['do_pretrained'],\n",
        "                                                  imagenet_w_init=global_var['imagenet_w_init'])\n",
        "        if global_var['do_print_model']:\n",
        "            print_model(model=model, device=device, save_model_root=save_model_root,\n",
        "                        input_shape=global_var['RGB_img_res'])\n",
        "        print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "    # Evaluate\n",
        "    print(' --- Begin evaluation --- ')\n",
        "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
        "    print(' --- End evaluation --- ')\n",
        "\n",
        "    if global_var['do_print_best_worst']:\n",
        "        sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
        "        save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
        "        save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Hardware\n",
        "    device = hardware_check()\n",
        "\n",
        "    # -- TRAIN 1\n",
        "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
        "    # Directory test\n",
        "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
        "    #print(save_model_root)\n",
        "    # Create folders\n",
        "    if global_var['do_train']:\n",
        "        if not os.path.exists(save_model_root):\n",
        "            os.makedirs(save_model_root)\n",
        "        # if not os.path.exists(save_model_root + 'info_code/'):\n",
        "        #     os.makedirs(save_model_root + 'info_code/')\n",
        "        # files_directory = '/work/project/'\n",
        "        # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
        "        # for f in files:\n",
        "        #     shutil.copy(f, save_model_root + 'info_code/')\n",
        "    # Run process\n",
        "    start_time = perf_counter()\n",
        "    process(device=device)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = perf_counter()\n",
        "    print(\"Total time elapsed: \",end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Times Test"
      ],
      "metadata": {
        "id": "3nzMYIWRNcAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meter_types = ['s','xs']\n",
        "test_rounds = 30\n",
        "blockPrint()\n",
        "device = hardware_check()\n",
        "enablePrint()\n",
        "\n",
        "if device == \"cpu\":\n",
        "  dev = !lscpu |grep 'Model name'\n",
        "  dev = str(dev).strip(\"]'\").split(\":\")[1].strip()\n",
        "  print(\"Experiment runned throught Colab CPU:\", dev)\n",
        "else:\n",
        "  dev = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  print(\"Experiment runned throught Colab GPU:\", dev[1])\n",
        "\n",
        "# Set-seed\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "np.random.seed(global_var['seed'])\n",
        "torch.cuda.manual_seed(global_var['seed'])\n",
        "\n",
        "# Datasets loading\n",
        "training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
        "      dts_type=global_var['dts_type'],\n",
        "      dts_root_path=dataset_root,\n",
        "      rgb_h_res=global_var['RGB_img_res'][1],\n",
        "      d_h_res=global_var['D_img_res'][1],\n",
        "      bs_train=global_var['batch_size'],\n",
        "      bs_eval=global_var['batch_size_eval'],\n",
        "      num_workers=global_var['n_workers'],\n",
        "      size_train=global_var['size_train'],\n",
        "      size_test=global_var['size_test']\n",
        "    )\n",
        "\n",
        "for arch_type in meter_types:\n",
        "  print(\"################################################################ TEST - %s architecture ################################################################\" % arch_type.upper())\n",
        "  times = np.ndarray(shape=test_rounds,dtype='float')\n",
        "\n",
        "  blockPrint()\n",
        "  model = build_model(device=device, arch_type=arch_type).to(device=device)\n",
        "  model, model_name = load_pretrained_model(model=model,\n",
        "                                            path_weigths=save_model_root + arch_type + '_build_model_best',\n",
        "                                            device=device,\n",
        "                                            do_pretrained=global_var['do_pretrained'],\n",
        "                                            imagenet_w_init=global_var['imagenet_w_init'])\n",
        "  enablePrint()\n",
        "\n",
        "\n",
        "  print(\"Model: %s\" % network_type)\n",
        "  blockPrint()\n",
        "  infos = summary(model,torch.ones(1,3,192,256).to(device))\n",
        "  enablePrint()\n",
        "  warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "  print(\"Trainable parameters: %d\" % count_parameters(model))\n",
        "  print(\"Mult-Adds: %d\" % int(infos[\"Mult-Adds\"].sum()))\n",
        "\n",
        "  for tests in range(test_rounds):\n",
        "      # Evaluate\n",
        "      blockPrint()\n",
        "      start_time = perf_counter()\n",
        "      best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
        "      #torch.cuda.synchronize() # <--------------------------------- REMEMBER WITH GPU\n",
        "      end_time = perf_counter()\n",
        "      enablePrint()\n",
        "\n",
        "      times[tests] = end_time - start_time\n",
        "\n",
        "  print(\"Average test time: \",np.mean(times))\n",
        "  print(\"Test times: \",times)"
      ],
      "metadata": {
        "id": "4UA4W-2CNe16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "fc2554ab-d7fb-4c58-a267-a205c045e806"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment runned throught Colab CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "################################################################ TEST - S architecture ################################################################\n",
            "Model: pyramidalMETER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 5527168\n",
            "Mult-Adds: 1002103728\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-60954266088a>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mblockPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mbest_worst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_DataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_save_csv_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_model_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0;31m#torch.cuda.synchronize() # <--------------------------------- REMEMBER WITH GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8fba2da9f604>\u001b[0m in \u001b[0;36mcompute_evaluation\u001b[0;34m(test_dataloader, model, model_type, path_save_csv_results)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0maverage_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (result, inputs.size(0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3639a8ebd971>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3639a8ebd971>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3639a8ebd971>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enablePrint()\n",
        "print(\"lukakuk la milan\")"
      ],
      "metadata": {
        "id": "wCPyollb-O_M"
      },
      "execution_count": 84,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BoQYEe4V5j5E",
        "8zUH-3na7eAH",
        "bbCnAwv453IN",
        "mLUvGTJF55VD",
        "tAWQQQzf8ctn",
        "eAvmPzvA8Pu-"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}