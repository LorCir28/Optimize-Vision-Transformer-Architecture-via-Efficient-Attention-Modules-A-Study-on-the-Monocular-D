{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g2bPw-5W4hJe"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb3YQA5HFeNz",
        "outputId": "133cab2d-cd5d-4a53-df6d-6cce2fc083c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyr3kLf61IIP"
      },
      "outputs": [],
      "source": [
        "global_var = {\n",
        "    # Resolutions\n",
        "    'RGB_img_res': (3, 192, 256),\n",
        "    'D_img_res': (1, 48, 64),\n",
        "    # Operations\n",
        "    'do_prints': False,\n",
        "    'do_print_model': True,\n",
        "    'do_pretrained': False,\n",
        "    'do_train': True,\n",
        "    'do_print_best_worst': True,\n",
        "    # ImageNet Initialization\n",
        "    'imagenet_w_init': False,\n",
        "    'imagenet_enc': 'METER_nyu_encoder_aug_mean0_var1_59', # A voi non interessa\n",
        "    # Parameters\n",
        "    'dts_type': 'nyu',\n",
        "    'architecture_type': 's',\n",
        "    'seed': 10000,\n",
        "    'lr': 1e-3,\n",
        "    'lr_patience': 15,\n",
        "    'epochs': 80, # Scegliete VOI,\n",
        "    'batch_size': 64,\n",
        "    'batch_size_eval': 1,\n",
        "    'n_workers': 2,\n",
        "    'e_stop_epochs': 30,\n",
        "    'size_train': None,\n",
        "    'size_test': None,\n",
        "}\n",
        "\n",
        "augmentation_parameters = {\n",
        "    'flip': 0.5,\n",
        "    'mirror': 0.5,\n",
        "    'color&bright': 0.5,\n",
        "    'c_swap': 0.5,\n",
        "    'random_crop': 0.5,\n",
        "    'random_d_shift': 0.5  # range(+-10)cm\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAAYJlZW4p9a"
      },
      "outputs": [],
      "source": [
        "# Paths ## DA CONTROLLARE CON DRIVE \"/content/drive/MyDrive/...\"\n",
        "dataset_root = '/content/drive/MyDrive/Visope_project/'\n",
        "save_model_root = '/content/drive/MyDrive/Visope_project/0_METER_Meta/PapaMetaMETER_s_results/'\n",
        "imagenet_init = '/work/imagenet/'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb6YUgKM5H_i"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVq5yvth7LJg",
        "outputId": "5c62893c-ddba-44cb-b24e-658f8b8bfb91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchsummaryX) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchsummaryX) (16.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchsummaryX) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchsummaryX) (1.3.0)\n",
            "Installing collected packages: einops, torchsummaryX\n",
            "Successfully installed einops-0.6.1 torchsummaryX-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOqnce-65LvH"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage.transform as st\n",
        "import torch\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummaryX import summary\n",
        "from scipy.interpolate import LinearNDInterpolator\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import random\n",
        "import torchvision.transforms as TT\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "from itertools import product\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import exp\n",
        "from einops import rearrange\n",
        "import csv\n",
        "import math\n",
        "from time import perf_counter\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zNWdsJdwlrh"
      },
      "outputs": [],
      "source": [
        "network_type = \"MetaMETER\"\n",
        "old_stout = sys.stdout"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2TAq6B5UBI"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s77jAM-X5VsY"
      },
      "outputs": [],
      "source": [
        "def hardware_check():\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Actual device: \", device)\n",
        "    if 'cuda' in device:\n",
        "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
        "\n",
        "    return device\n",
        "\n",
        "\n",
        "def plot_depth_map(dm):\n",
        "\n",
        "    MIN_DEPTH = 0.0\n",
        "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
        "\n",
        "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
        "    cmap = plt.cm.plasma_r\n",
        "\n",
        "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
        "\n",
        "\n",
        "def resize_keeping_aspect_ratio(img, base):\n",
        "    \"\"\"\n",
        "    Resize the image to a defined length manteining its proportions\n",
        "    Scaling the shortest side of the image to a fixed 'base' length'\n",
        "    \"\"\"\n",
        "\n",
        "    if img.shape[0] <= img.shape[1]:\n",
        "        basewidth = int(base)\n",
        "        wpercent = (basewidth / float(img.shape[0]))\n",
        "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
        "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
        "    else:\n",
        "        baseheight = int(base)\n",
        "        wpercent = (baseheight / float(img.shape[1]))\n",
        "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
        "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def compute_rmse(predictions, depths):\n",
        "    valid_mask = depths > 0.0\n",
        "    valid_predictions = predictions[valid_mask]\n",
        "    valid_depths = depths[valid_mask]\n",
        "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
        "    return torch.sqrt(mse)\n",
        "\n",
        "\n",
        "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
        "    valid_mask = y_true > 0.0\n",
        "    valid_pred = y_pred[valid_mask]\n",
        "    valid_true = y_true[valid_mask]\n",
        "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
        "    return 100 * torch.mean(correct.float())\n",
        "\n",
        "\n",
        "def print_model(model, device, save_model_root, input_shape):\n",
        "    info = summary(model, torch.ones((1, input_shape[0], input_shape[1], input_shape[2])).to(device))\n",
        "    info.to_csv(save_model_root + 'model_summary.csv')\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "def save_checkpoint(model, name, path_save_model):\n",
        "    \"\"\"\n",
        "    Saves a model\n",
        "    \"\"\"\n",
        "    if '_best' in name:\n",
        "        folder = name.split(\"_best\")[0]\n",
        "    elif '_checkpoint' in name:\n",
        "        folder = name.split(\"_checkpoint\")[0]\n",
        "    if not os.path.isdir(path_save_model):\n",
        "        os.makedirs(path_save_model, exist_ok=True)\n",
        "    torch.save(model.state_dict(), path_save_model + name)\n",
        "\n",
        "\n",
        "def save_history(history, filepath):\n",
        "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
        "    pickle.dump(history, tmp_file)\n",
        "    tmp_file.close()\n",
        "\n",
        "\n",
        "def save_csv_history(model_name, path):\n",
        "    objects = []\n",
        "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    df = pd.DataFrame(objects)\n",
        "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
        "\n",
        "\n",
        "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
        "    model_name = model.__class__.__name__\n",
        "\n",
        "    if do_pretrained:\n",
        "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
        "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
        "        model.load_state_dict(model_dict)\n",
        "        print(\"checkpoint loaded\\n\")\n",
        "\n",
        "    if imagenet_w_init:\n",
        "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
        "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
        "        model_dict = model.state_dict()\n",
        "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
        "\n",
        "        # pretrained_param = len(pretrained_dict.items())\n",
        "        counter_param = 0\n",
        "        for i, j in pretrained_dict.items():\n",
        "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
        "                counter_param += 1\n",
        "\n",
        "        print(f'Pertained parameters: {counter_param}\\n')\n",
        "\n",
        "        # 1. filter out unnecessary keys\n",
        "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
        "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
        "        # 2. overwrite entries in the existing state dict\n",
        "        model_dict.update(pretrained_dict)\n",
        "        # 3. load the new state dict\n",
        "        model.load_state_dict(model_dict)\n",
        "\n",
        "        # alternativa to 2 e 3\n",
        "        # model.load_state_dict(pretrained_dict, strict=False)\n",
        "        print(\"Partial initialization computed\\n\")\n",
        "\n",
        "    return model, model_name\n",
        "\n",
        "\n",
        "def plot_graph(f, g, f_label, g_label, title, path):\n",
        "    epochs = range(0, len(f))\n",
        "    plt.plot(epochs, f, 'b', label=f_label)\n",
        "    plt.plot(epochs, g, 'orange', label=g_label)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid('on', color='#cfcfcf')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + title + '.pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_history(history, path):\n",
        "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
        "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
        "\n",
        "\n",
        "def plot_loss_parts(history, path, title):\n",
        "    l_mae_list = history['l_mae']\n",
        "    l_norm_list = history['l_norm']\n",
        "    l_grad_list = history['l_grad']\n",
        "    l_ssim_list = history['l_ssim']\n",
        "    epochs = range(0, len(l_mae_list))\n",
        "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
        "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
        "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
        "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.grid('on', color='#cfcfcf')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + title + '.pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
        "    for i in range(quantity):\n",
        "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
        "\n",
        "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
        "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
        "              f' variance =  {torch.var(img)}\\n')\n",
        "\n",
        "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title('Input image')\n",
        "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "        if not False:\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title('Grayscale DepthMap')\n",
        "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        if not False:\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title('Colored DepthMap')\n",
        "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
        "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        if not False:\n",
        "            plt.axis('off')\n",
        "\n",
        "        print(\"************************** \",save_model_root)\n",
        "        save_path = save_model_root + 'example&augment_img/'\n",
        "        print(\"************************** \",save_path)\n",
        "        if not os.path.exists(save_path):\n",
        "            os.mkdir(save_path)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
        "        plt.close(fig=fig)\n",
        "\n",
        "\n",
        "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
        "    \"\"\"\n",
        "    Shows prediction example\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
        "    for i, index in zip(range(len(indices)), indices):\n",
        "        img, depth = dataset.__getitem__(index)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        # Predict\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(torch.from_numpy(img).to(device))\n",
        "            # Build plot\n",
        "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
        "            plt.subplot(1, len(indices), i+1)\n",
        "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
        "            cbar = plt.colorbar()\n",
        "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
        "            if False:\n",
        "                plt.axis('off')\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
        "    plt.close(fig=fig)\n",
        "\n",
        "\n",
        "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
        "    save_path = save_model_root + type + '_predictions/'\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for i in range(len(list_type)):\n",
        "        index_image = list_type[i][0]\n",
        "        rmse_value = list_type[i][1]\n",
        "\n",
        "        img, depth = dataset.__getitem__(index=index_image)\n",
        "\n",
        "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.title(f'Original image {index_image}')\n",
        "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.title('Ground Truth')\n",
        "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
        "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predict\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
        "\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.title('Predicted DepthMap')\n",
        "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
        "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
        "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
        "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
        "        plt.close(fig=fig)\n",
        "\n",
        "\n",
        "def compute_MeanVar(dataset):\n",
        "    r_mean, g_mean, b_mean = [], [], []\n",
        "    r_var, g_var, b_var = [], [], []\n",
        "    for i in range(dataset.__len__()):\n",
        "        img, _ = dataset.__getitem__(index=i)\n",
        "        r = np.array(img[0, :, :])\n",
        "        g = np.array(img[1, :, :])\n",
        "        b = np.array(img[2, :, :])\n",
        "\n",
        "        r_mean.append(np.mean(r))\n",
        "        g_mean.append(np.mean(g))\n",
        "        b_mean.append(np.mean(b))\n",
        "\n",
        "        r_var.append(np.var(r))\n",
        "        g_var.append(np.var(g))\n",
        "        b_var.append(np.var(b))\n",
        "\n",
        "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
        "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
        "\n",
        "\n",
        "def compute_MeanImg(dataset, save_model_root):\n",
        "    r, g, b = [], [], []\n",
        "    for i in range(dataset.__len__()):\n",
        "        img, _ = dataset.__getitem__(index=i)\n",
        "        r.append(np.array(img[0, :, :]))\n",
        "        g.append(np.array(img[1, :, :]))\n",
        "        b.append(np.array(img[2, :, :]))\n",
        "\n",
        "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
        "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
        "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
        "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
        "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
        "\n",
        "    print(\"Process Completed\")\n",
        "\n",
        "def blockPrint():\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "# Restore\n",
        "def enablePrint():\n",
        "    sys.stdout = old_stout"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQYEe4V5j5E"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8zUH-3na7eAH"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFEYqVd7i1H"
      },
      "outputs": [],
      "source": [
        "def pixel_shift(depth_img, shift):\n",
        "    depth_img = depth_img + shift\n",
        "    return depth_img\n",
        "\n",
        "\n",
        "def random_crop(x, y, crop_size=(192, 256)):\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    assert x.shape[1] == y.shape[1]\n",
        "    h, w, _ = x.shape\n",
        "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
        "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
        "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
        "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
        "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
        "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
        "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
        "    if cropped_y.shape[-1] == 0:\n",
        "        return x, y\n",
        "    else:\n",
        "        return cropped_x, cropped_y\n",
        "\n",
        "\n",
        "def augmentation2D(img, depth, print_info_aug):\n",
        "    # Random flipping\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
        "        img = (img[..., ::1, :, :]).copy()\n",
        "        depth = (depth[..., ::1, :, :]).copy()\n",
        "        if print_info_aug:\n",
        "            print('--> Random flipped')\n",
        "    # Random mirroring\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
        "        img = (img[..., ::-1, :]).copy()\n",
        "        depth = (depth[..., ::-1, :]).copy()\n",
        "        if print_info_aug:\n",
        "            print('--> Random mirrored')\n",
        "    # Augment image\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
        "        # gamma augmentation\n",
        "        gamma = random.uniform(0.9, 1.1)\n",
        "        img = img ** gamma\n",
        "        brightness = random.uniform(0.9, 1.1)\n",
        "        img = img * brightness\n",
        "        # color augmentation\n",
        "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
        "        white = np.ones((img.shape[0], img.shape[1]))\n",
        "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
        "        img *= color_image\n",
        "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
        "        if print_info_aug:\n",
        "            print('--> Image randomly augmented')\n",
        "    # Channel swap\n",
        "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
        "        indices = list(product([0, 1, 2], repeat=3))\n",
        "        policy_idx = random.randint(0, len(indices) - 1)\n",
        "        img = img[..., list(indices[policy_idx])]\n",
        "        if print_info_aug:\n",
        "            print('--> Channel swapped')\n",
        "    # Random crop\n",
        "    if random.random() <= augmentation_parameters['random_crop']:\n",
        "        img, depth = random_crop(img, depth)\n",
        "        if print_info_aug:\n",
        "            print('--> Random cropped')\n",
        "    # Depth Shift\n",
        "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
        "        random_shift = random.randint(-10, 10)\n",
        "        depth = pixel_shift(depth, shift=random_shift)\n",
        "        if print_info_aug:\n",
        "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
        "\n",
        "    return img, depth"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bbCnAwv453IN"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhuQQhwf597g"
      },
      "outputs": [],
      "source": [
        "class NYU2_Dataset:\n",
        "    \"\"\"\n",
        "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
        "      * 654 Test and 50688 Train images\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
        "        self.dataset = path\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.info = 0\n",
        "        self.dts_type = dts_type\n",
        "        self.aug = aug\n",
        "        self.rgb_h_res = rgb_h_res\n",
        "        self.d_h_res = d_h_res\n",
        "        self.scenarios = scenarios\n",
        "\n",
        "        # Handle dataset\n",
        "        if self.dts_type == 'test':\n",
        "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
        "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
        "\n",
        "            rgb = np.load(img_path)\n",
        "            depth = np.load(depth_path)\n",
        "\n",
        "            self.x = rgb\n",
        "            self.y = depth\n",
        "\n",
        "            if dts_size != 0:\n",
        "                self.x = rgb[:dts_size]\n",
        "                self.y = depth[:dts_size]\n",
        "\n",
        "            self.info = len(self.x)\n",
        "\n",
        "        elif self.dts_type == 'train':\n",
        "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
        "            for scene in scenarios:\n",
        "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
        "                for el in elem:\n",
        "                    if 'jpg' in el:\n",
        "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
        "                    elif 'png' in el:\n",
        "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
        "                    else:\n",
        "                        raise SystemError('Type image error (train)')\n",
        "\n",
        "            if len(self.x) != len(self.y):\n",
        "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
        "\n",
        "            self.x.sort()\n",
        "            self.y.sort()\n",
        "\n",
        "            if dts_size != 0:\n",
        "                self.x = self.x[:dts_size]\n",
        "                self.y = self.y[:dts_size]\n",
        "\n",
        "            self.info = len(self.x)\n",
        "\n",
        "        else:\n",
        "            raise SystemError('Problem in the path')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.info\n",
        "\n",
        "    def __getitem__(self, index=None, print_info_aug=False):\n",
        "        if index is None:\n",
        "            index = np.random.randint(0, self.info)\n",
        "\n",
        "        # Load Image\n",
        "        if self.dts_type == 'test':\n",
        "            img = self.x[index]\n",
        "        else:\n",
        "            img = Image.open(self.dataset + self.x[index]).convert('RGB')\n",
        "            img = np.array(img)\n",
        "\n",
        "        # Load Depth Image\n",
        "        if self.dts_type == 'test':\n",
        "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
        "        else:\n",
        "            depth = Image.open(self.dataset + self.y[index])\n",
        "            depth = np.array(depth) / 255\n",
        "            depth = np.clip(depth * 1000, 50, 1000)\n",
        "            depth = np.expand_dims(depth, axis=-1)\n",
        "\n",
        "        # Augmentation\n",
        "        if self.aug:\n",
        "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
        "\n",
        "        img_post_processing = TT.Compose([\n",
        "            TT.ToTensor(),\n",
        "            TT.Resize((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), antialias=True),\n",
        "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
        "        ])\n",
        "        depth_post_processing = TT.Compose([\n",
        "            TT.ToTensor(),\n",
        "            TT.Resize((global_var['D_img_res'][1], global_var['D_img_res'][2]), antialias=True),\n",
        "        ])\n",
        "\n",
        "        img = img_post_processing(img/255)\n",
        "        depth = depth_post_processing(depth)\n",
        "\n",
        "        return img.float(), depth.float()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mLUvGTJF55VD"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKUmyNBL5m1o"
      },
      "outputs": [],
      "source": [
        "def init_train_test_loader(dts_type, dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
        "    if dts_type == 'nyu':\n",
        "        Dataset_class = NYU2_Dataset\n",
        "        dts_root_path = dts_root_path + 'NYUv2/'\n",
        "    else:\n",
        "        print('OCCHIO AL DATASET')\n",
        "\n",
        "\n",
        "    # Load Datasets\n",
        "    test_Dataset = Dataset_class(\n",
        "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
        "    )\n",
        "    training_Dataset = Dataset_class(\n",
        "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
        "    )\n",
        "    # Create Dataloaders\n",
        "    training_DataLoader = DataLoader(\n",
        "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
        "    )\n",
        "    test_DataLoader = DataLoader(\n",
        "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tAWQQQzf8ctn"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTaUymYS8mDJ"
      },
      "outputs": [],
      "source": [
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
        "    L = val_range\n",
        "\n",
        "    padd = 0\n",
        "    (_, channel, height, width) = img1.size()\n",
        "    if window is None:\n",
        "        real_size = min(window_size, height, width)\n",
        "        window = create_window(real_size, channel=channel).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * L) ** 2\n",
        "    C2 = (0.03 * L) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "    if size_average:\n",
        "        ret = ssim_map.mean()\n",
        "    else:\n",
        "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "    if full:\n",
        "        return ret, cs\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "class Sobel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sobel, self).__init__()\n",
        "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
        "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
        "        edge_k = np.stack((edge_kx, edge_ky))\n",
        "\n",
        "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
        "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.edge_conv(x)\n",
        "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class balanced_loss_function(nn.Module):\n",
        "\n",
        "    def __init__(self, device):\n",
        "        super(balanced_loss_function, self).__init__()\n",
        "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
        "        self.get_gradient = Sobel().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, output, depth):\n",
        "        with torch.no_grad():\n",
        "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
        "\n",
        "        depth_grad = self.get_gradient(depth)\n",
        "        output_grad = self.get_gradient(output)\n",
        "\n",
        "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
        "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
        "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
        "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
        "\n",
        "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
        "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
        "\n",
        "        loss_depth = torch.abs(output - depth).mean()\n",
        "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
        "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
        "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
        "\n",
        "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
        "\n",
        "        loss_grad = (loss_dx + loss_dy) / 2\n",
        "\n",
        "        return loss_depth, loss_ssim, loss_normal, loss_grad"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vhLCflR28fxo"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9_rdzyp87dB"
      },
      "outputs": [],
      "source": [
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU()  # nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   groups=depth,\n",
        "                                   padding=1,\n",
        "                                   stride=stride,\n",
        "                                   bias=bias).to(device)\n",
        "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
        "    return nn.Sequential(\n",
        "        # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
        "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
        "                        #bias=False, device='cuda:0'),\n",
        "                        bias=False, device='cpu'),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU()  # nn.SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class ModLayerNorm(nn.GroupNorm):\n",
        "  def __init__(self, dim):\n",
        "      super().__init__(1, dim)\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = ModLayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv2d(dim,hidden_dim,1)\n",
        "        self.act = nn.ReLU()\n",
        "        self.c2 = nn.Conv2d(hidden_dim,dim,1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.c2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, pool_size):\n",
        "      super().__init__()\n",
        "      self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #print(\"Pre pool: \", x.shape)\n",
        "      #print(\"Post pool: \", self.pool(x).shape)\n",
        "      return self.pool(x) - x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                #PreNorm(dim, Attention(3)),\n",
        "                PreNorm(dim, Attention(1)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            #print(\"***************************** Start logging *****************************\")\n",
        "            #print(\"Input shape: \", x.shape)\n",
        "            x = attn(x) + x\n",
        "            #print(\"Post attention shape: \", x.shape)\n",
        "            #print(\"FF values-> dim:%d, mlp_dim: %2d\" % (240, 120))\n",
        "            x = ff(x) + x\n",
        "            #print(\"Post MLP shape: \", x.shape)\n",
        "            #print(\"***************************** End logging *****************************\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(inp * expansion)\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU(),  # nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU(),  # nn.SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU(),  # nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)  # Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
        "\n",
        "        self.conv3 = conv_1x1_bn(dim, channel)\n",
        "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        #B, C, H, W = Batch, Channels, Height, Width\n",
        "        # Global representations\n",
        "        _, _, h, w = x.shape\n",
        "        # b = batch\n",
        "        # d = depth = channels ?\n",
        "        # h = height\n",
        "        # ph = ?\n",
        "        # pw = ?\n",
        "\n",
        "        start_time = perf_counter() ############################## Time measurament\n",
        "        x = self.transformer(x)\n",
        "        end_time = perf_counter() ############################## Time measurament\n",
        "\n",
        "        # Fusion\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat((x, y), 1)\n",
        "        x = self.conv4(x)\n",
        "        return x, end_time-start_time ############################## Time measurament\n",
        "\n",
        "\n",
        "class MobileViT(nn.Module):\n",
        "    def __init__(self, image_size, dims, channels, num_classes,transformer_times, sample_cnt,expansion=4, kernel_size=3, patch_size=(2, 2)): ############################## Time measurament\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        ph, pw = patch_size\n",
        "        assert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "        self.transformer_times = transformer_times ############################## Time measurament\n",
        "        self.sample_cnt = sample_cnt ############################## Time measurament\n",
        "\n",
        "        L = [1, 1, 1]  # L = [2, 4, 3] # --> +5 FPS\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
        "\n",
        "        self.mv2 = nn.ModuleList([])\n",
        "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
        "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "\n",
        "        self.mvit = nn.ModuleList([])\n",
        "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
        "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
        "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
        "\n",
        "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "        # self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        # self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y0 = self.conv1(x)\n",
        "        x = self.mv2[0](y0)\n",
        "\n",
        "        y1 = self.mv2[1](x)\n",
        "        x = self.mv2[2](y1)\n",
        "        x = self.mv2[3](x)  # Repeat\n",
        "\n",
        "        y2 = self.mv2[4](x)\n",
        "        x,mvit_time_1 = self.mvit[0](y2)\n",
        "        self.transformer_times[0][self.sample_cnt] = mvit_time_1 ############################## Time measurament\n",
        "\n",
        "        y3 = self.mv2[5](x)\n",
        "        x,mvit_time_2 = self.mvit[1](y3)\n",
        "        self.transformer_times[1][self.sample_cnt] = mvit_time_2 ############################## Time measurament\n",
        "\n",
        "        x = self.mv2[6](x)\n",
        "        x,mvit_time_3 = self.mvit[2](x)\n",
        "        self.transformer_times[2][self.sample_cnt] = mvit_time_3 ############################## Time measurament\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        self.sample_cnt += 1 ############################## Time measurament\n",
        "        if(self.sample_cnt == 655):\n",
        "          self.sample_cnt = 0\n",
        "\n",
        "        return x, [y0, y1, y2, y3]\n",
        "\n",
        "\n",
        "def mobilevit_xxs(transformer_times, sample_cnt): ############################## Time measurament\n",
        "    enc_type = 'xxs'\n",
        "    dims = [64, 80, 96]\n",
        "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]  # 320\n",
        "    return MobileViT((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), dims, channels, num_classes=1000, expansion=2,\n",
        "                     transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
        "\n",
        "\n",
        "def mobilevit_xs(transformer_times, sample_cnt): ############################## Time measurament\n",
        "    enc_type = 'xs'\n",
        "    dims = [96, 120, 144]\n",
        "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 192] # 384\n",
        "    return MobileViT((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), dims, channels, num_classes=1000,\n",
        "                     transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
        "\n",
        "\n",
        "def mobilevit_s(transformer_times, sample_cnt): ############################## Time measurament\n",
        "    enc_type = 's'\n",
        "    dims = [144, 192, 240]\n",
        "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
        "    return MobileViT((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]), dims, channels, num_classes=1000,\n",
        "                     transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
        "\n",
        "\n",
        "class UpSample_layer(nn.Module):\n",
        "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
        "        super(UpSample_layer, self).__init__()\n",
        "        self.flag = flag\n",
        "        self.name = name\n",
        "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
        "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
        "        self.end_up_layer = nn.Sequential(\n",
        "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_layer):\n",
        "        x = self.conv2d_transpose(x)\n",
        "        if x.shape[-1] != enc_layer.shape[-1]:\n",
        "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n",
        "        if x.shape[-1] != enc_layer.shape[-1]:\n",
        "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n",
        "        x = torch.cat([x, enc_layer], dim=1)\n",
        "        x = self.end_up_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SPEED_decoder(nn.Module):\n",
        "    def __init__(self, device, typ):\n",
        "        super(SPEED_decoder, self).__init__()\n",
        "        self.conv2d_in = nn.Conv2d(320 if typ == 's' else 192 if typ == 'xs' else 160,\n",
        "                                   128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
        "                                   kernel_size=(1, 1), padding='same', bias=False)\n",
        "        self.ups_block_1 = UpSample_layer(128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
        "                                          64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
        "                                          flag=True,\n",
        "                                          sep_conv_filters=192 if typ == 's' else 144 if typ == 'xs' else 96,\n",
        "                                          name='up1', device=device)\n",
        "        self.ups_block_2 = UpSample_layer(64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
        "                                          32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
        "                                          flag=False,\n",
        "                                          sep_conv_filters=128 if typ == 's' else 96 if typ == 'xs' else 64,\n",
        "                                          name='up2', device=device)\n",
        "        self.ups_block_3 = UpSample_layer(32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
        "                                          16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
        "                                          flag=False,\n",
        "                                          sep_conv_filters=80 if typ == 's' else 64 if typ == 'xs' else 32,\n",
        "                                          name='up3', device=device)\n",
        "        self.conv2d_out = nn.Conv2d(16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
        "                                    1, kernel_size=(3, 3), padding='same', bias=False)\n",
        "\n",
        "    def forward(self, x, enc_layer_list):\n",
        "        x = self.conv2d_in(x)\n",
        "        x = self.ups_block_1(x, enc_layer_list[3])\n",
        "        x = self.ups_block_2(x, enc_layer_list[2])\n",
        "        x = self.ups_block_3(x, enc_layer_list[1])\n",
        "        x = self.conv2d_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class build_model(nn.Module):\n",
        "    \"\"\"\n",
        "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, device, arch_type):\n",
        "        super(build_model, self).__init__()\n",
        "        self.transformer_times = np.zeros((3,655),dtype='float') ############################## Time measurament\n",
        "        self.sample_cnt = 0 ############################## Time measurament\n",
        "\n",
        "        if arch_type == 's':\n",
        "            self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
        "        elif arch_type == 'xs':\n",
        "            self.encoder, enc_type = mobilevit_xs(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
        "        else:\n",
        "            self.encoder, enc_type = mobilevit_xxs(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
        "        self.decoder = SPEED_decoder(device=device, typ=enc_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, enc_layer = self.encoder(x)\n",
        "        x = self.decoder(x, enc_layer)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eAvmPzvA8Pu-"
      },
      "source": [
        "# Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es_aqA008Sof"
      },
      "outputs": [],
      "source": [
        "def log10(x):\n",
        "    return torch.log(x) / math.log(10)\n",
        "\n",
        "\n",
        "class Result(object):\n",
        "    def __init__(self):\n",
        "        self.irmse, self.imae = 0, 0\n",
        "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
        "        self.absrel, self.lg10 = 0, 0\n",
        "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
        "\n",
        "    def set_to_worst(self):\n",
        "        self.irmse, self.imae = np.inf, np.inf\n",
        "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
        "        self.absrel, self.lg10 = np.inf, np.inf\n",
        "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
        "\n",
        "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
        "        self.irmse, self.imae = irmse, imae\n",
        "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
        "        self.absrel, self.lg10 = absrel, lg10\n",
        "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
        "\n",
        "    def evaluate(self, output, target):\n",
        "        valid_mask = target > 0\n",
        "\n",
        "        output = output[valid_mask]\n",
        "        target = target[valid_mask]\n",
        "\n",
        "        if 'kitti' in global_var['dts_type']:\n",
        "            output = output[2080:] # remove first 13pixels lines\n",
        "            target = target[2080:]\n",
        "\n",
        "        abs_diff = (output - target).abs()\n",
        "\n",
        "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
        "        self.rmse = math.sqrt(self.mse)\n",
        "        self.mae = float(abs_diff.mean())\n",
        "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
        "        self.absrel = float((abs_diff / target).mean())\n",
        "\n",
        "        maxRatio = torch.max(output / target, target / output)\n",
        "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
        "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
        "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
        "\n",
        "        inv_output = 1 / output\n",
        "        inv_target = 1 / target\n",
        "        abs_inv_diff = (inv_output - inv_target).abs()\n",
        "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
        "        self.imae = float(abs_inv_diff.mean())\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.count = 0.0\n",
        "        self.sum_irmse, self.sum_imae = 0, 0\n",
        "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
        "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
        "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
        "\n",
        "    def update(self, result, n=1):\n",
        "        self.count += n\n",
        "\n",
        "        self.sum_irmse += n * result.irmse\n",
        "        self.sum_imae += n * result.imae\n",
        "        self.sum_mse += n * result.mse\n",
        "        self.sum_rmse += n * result.rmse\n",
        "        self.sum_mae += n * result.mae\n",
        "        self.sum_absrel += n * result.absrel\n",
        "        self.sum_lg10 += n * result.lg10\n",
        "        self.sum_delta1 += n * result.delta1\n",
        "        self.sum_delta2 += n * result.delta2\n",
        "        self.sum_delta3 += n * result.delta3\n",
        "\n",
        "    def average(self):\n",
        "        avg = Result()\n",
        "        avg.update(\n",
        "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
        "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
        "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
        "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
        "        return avg\n",
        "\n",
        "\n",
        "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
        "    best_worst_dict = {}\n",
        "    result = Result()\n",
        "    result.set_to_worst()\n",
        "    average_meter = AverageMeter()\n",
        "    model.eval()  # switch to evaluate mode\n",
        "\n",
        "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
        "        #inputs, depths = inputs.cuda(), depths.cuda()\n",
        "        inputs, depths = inputs.cpu(), depths.cpu()\n",
        "        # compute output\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs)\n",
        "        result.evaluate(predictions, depths)\n",
        "        average_meter.update(result)  # (result, inputs.size(0))\n",
        "        best_worst_dict[i] = result.rmse\n",
        "\n",
        "    avg = average_meter.average()\n",
        "\n",
        "    print('MAE={average.mae:.3f}\\n'\n",
        "          'RMSE={average.rmse:.3f}\\n'\n",
        "          'Delta1={average.delta1:.3f}\\n'\n",
        "          'REL={average.absrel:.3f}\\n'\n",
        "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
        "\n",
        "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
        "        writer.writeheader()\n",
        "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
        "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
        "\n",
        "    return best_worst_dict, avg"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EjiqGK4q42zP"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "590sGQdh45FS",
        "outputId": "14b1c02e-ada3-4ba5-8e8c-4d5be04cc13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual device:  cpu\n",
            "INFO: There are 2312 training and 654 testing samples\n",
            "=======================================================================================================================\n",
            "                                                        Kernel Shape  \\\n",
            "Layer                                                                  \n",
            "0_encoder.conv1.0.Conv2d_depthwise                     [3, 16, 3, 3]   \n",
            "1_encoder.conv1.0.Conv2d_pointwise                    [16, 16, 1, 1]   \n",
            "2_encoder.conv1.BatchNorm2d_1                                   [16]   \n",
            "3_encoder.conv1.ReLU_2                                             -   \n",
            "4_encoder.mv2.0.conv.Conv2d_0                         [16, 64, 1, 1]   \n",
            "5_encoder.mv2.0.conv.BatchNorm2d_1                              [64]   \n",
            "6_encoder.mv2.0.conv.ReLU_2                                        -   \n",
            "7_encoder.mv2.0.conv.Conv2d_3                          [1, 64, 3, 3]   \n",
            "8_encoder.mv2.0.conv.BatchNorm2d_4                              [64]   \n",
            "9_encoder.mv2.0.conv.ReLU_5                                        -   \n",
            "10_encoder.mv2.0.conv.Conv2d_6                        [64, 32, 1, 1]   \n",
            "11_encoder.mv2.0.conv.BatchNorm2d_7                             [32]   \n",
            "12_encoder.mv2.1.conv.Conv2d_0                       [32, 128, 1, 1]   \n",
            "13_encoder.mv2.1.conv.BatchNorm2d_1                            [128]   \n",
            "14_encoder.mv2.1.conv.ReLU_2                                       -   \n",
            "15_encoder.mv2.1.conv.Conv2d_3                        [1, 128, 3, 3]   \n",
            "16_encoder.mv2.1.conv.BatchNorm2d_4                            [128]   \n",
            "17_encoder.mv2.1.conv.ReLU_5                                       -   \n",
            "18_encoder.mv2.1.conv.Conv2d_6                       [128, 64, 1, 1]   \n",
            "19_encoder.mv2.1.conv.BatchNorm2d_7                             [64]   \n",
            "20_encoder.mv2.2.conv.Conv2d_0                       [64, 256, 1, 1]   \n",
            "21_encoder.mv2.2.conv.BatchNorm2d_1                            [256]   \n",
            "22_encoder.mv2.2.conv.ReLU_2                                       -   \n",
            "23_encoder.mv2.2.conv.Conv2d_3                        [1, 256, 3, 3]   \n",
            "24_encoder.mv2.2.conv.BatchNorm2d_4                            [256]   \n",
            "25_encoder.mv2.2.conv.ReLU_5                                       -   \n",
            "26_encoder.mv2.2.conv.Conv2d_6                       [256, 64, 1, 1]   \n",
            "27_encoder.mv2.2.conv.BatchNorm2d_7                             [64]   \n",
            "28_encoder.mv2.3.conv.Conv2d_0                       [64, 256, 1, 1]   \n",
            "29_encoder.mv2.3.conv.BatchNorm2d_1                            [256]   \n",
            "30_encoder.mv2.3.conv.ReLU_2                                       -   \n",
            "31_encoder.mv2.3.conv.Conv2d_3                        [1, 256, 3, 3]   \n",
            "32_encoder.mv2.3.conv.BatchNorm2d_4                            [256]   \n",
            "33_encoder.mv2.3.conv.ReLU_5                                       -   \n",
            "34_encoder.mv2.3.conv.Conv2d_6                       [256, 64, 1, 1]   \n",
            "35_encoder.mv2.3.conv.BatchNorm2d_7                             [64]   \n",
            "36_encoder.mv2.4.conv.Conv2d_0                       [64, 256, 1, 1]   \n",
            "37_encoder.mv2.4.conv.BatchNorm2d_1                            [256]   \n",
            "38_encoder.mv2.4.conv.ReLU_2                                       -   \n",
            "39_encoder.mv2.4.conv.Conv2d_3                        [1, 256, 3, 3]   \n",
            "40_encoder.mv2.4.conv.BatchNorm2d_4                            [256]   \n",
            "41_encoder.mv2.4.conv.ReLU_5                                       -   \n",
            "42_encoder.mv2.4.conv.Conv2d_6                       [256, 96, 1, 1]   \n",
            "43_encoder.mv2.4.conv.BatchNorm2d_7                             [96]   \n",
            "44_encoder.mvit.0.conv1.0.Conv2d_depthwise            [96, 96, 3, 3]   \n",
            "45_encoder.mvit.0.conv1.0.Conv2d_pointwise            [96, 96, 1, 1]   \n",
            "46_encoder.mvit.0.conv1.BatchNorm2d_1                           [96]   \n",
            "47_encoder.mvit.0.conv1.ReLU_2                                     -   \n",
            "48_encoder.mvit.0.conv2.Conv2d_0                     [96, 144, 1, 1]   \n",
            "49_encoder.mvit.0.conv2.BatchNorm2d_1                          [144]   \n",
            "50_encoder.mvit.0.conv2.ReLU_2                                     -   \n",
            "51_encoder.mvit.0.transformer.layers.0.0.ModLay...             [144]   \n",
            "52_encoder.mvit.0.transformer.layers.0.0.fn.Avg...                 -   \n",
            "53_encoder.mvit.0.transformer.layers.0.1.ModLay...             [144]   \n",
            "54_encoder.mvit.0.transformer.layers.0.1.fn.Con...  [144, 288, 1, 1]   \n",
            "55_encoder.mvit.0.transformer.layers.0.1.fn.ReL...                 -   \n",
            "56_encoder.mvit.0.transformer.layers.0.1.fn.Dro...                 -   \n",
            "57_encoder.mvit.0.transformer.layers.0.1.fn.Con...  [288, 144, 1, 1]   \n",
            "58_encoder.mvit.0.transformer.layers.0.1.fn.Dro...                 -   \n",
            "59_encoder.mvit.0.conv3.Conv2d_0                     [144, 96, 1, 1]   \n",
            "60_encoder.mvit.0.conv3.BatchNorm2d_1                           [96]   \n",
            "61_encoder.mvit.0.conv3.ReLU_2                                     -   \n",
            "62_encoder.mvit.0.conv4.0.Conv2d_depthwise           [192, 96, 3, 3]   \n",
            "63_encoder.mvit.0.conv4.0.Conv2d_pointwise            [96, 96, 1, 1]   \n",
            "64_encoder.mvit.0.conv4.BatchNorm2d_1                           [96]   \n",
            "65_encoder.mvit.0.conv4.ReLU_2                                     -   \n",
            "66_encoder.mv2.5.conv.Conv2d_0                       [96, 384, 1, 1]   \n",
            "67_encoder.mv2.5.conv.BatchNorm2d_1                            [384]   \n",
            "68_encoder.mv2.5.conv.ReLU_2                                       -   \n",
            "69_encoder.mv2.5.conv.Conv2d_3                        [1, 384, 3, 3]   \n",
            "70_encoder.mv2.5.conv.BatchNorm2d_4                            [384]   \n",
            "71_encoder.mv2.5.conv.ReLU_5                                       -   \n",
            "72_encoder.mv2.5.conv.Conv2d_6                      [384, 128, 1, 1]   \n",
            "73_encoder.mv2.5.conv.BatchNorm2d_7                            [128]   \n",
            "74_encoder.mvit.1.conv1.0.Conv2d_depthwise          [128, 128, 3, 3]   \n",
            "75_encoder.mvit.1.conv1.0.Conv2d_pointwise          [128, 128, 1, 1]   \n",
            "76_encoder.mvit.1.conv1.BatchNorm2d_1                          [128]   \n",
            "77_encoder.mvit.1.conv1.ReLU_2                                     -   \n",
            "78_encoder.mvit.1.conv2.Conv2d_0                    [128, 192, 1, 1]   \n",
            "79_encoder.mvit.1.conv2.BatchNorm2d_1                          [192]   \n",
            "80_encoder.mvit.1.conv2.ReLU_2                                     -   \n",
            "81_encoder.mvit.1.transformer.layers.0.0.ModLay...             [192]   \n",
            "82_encoder.mvit.1.transformer.layers.0.0.fn.Avg...                 -   \n",
            "83_encoder.mvit.1.transformer.layers.0.1.ModLay...             [192]   \n",
            "84_encoder.mvit.1.transformer.layers.0.1.fn.Con...  [192, 768, 1, 1]   \n",
            "85_encoder.mvit.1.transformer.layers.0.1.fn.ReL...                 -   \n",
            "86_encoder.mvit.1.transformer.layers.0.1.fn.Dro...                 -   \n",
            "87_encoder.mvit.1.transformer.layers.0.1.fn.Con...  [768, 192, 1, 1]   \n",
            "88_encoder.mvit.1.transformer.layers.0.1.fn.Dro...                 -   \n",
            "89_encoder.mvit.1.conv3.Conv2d_0                    [192, 128, 1, 1]   \n",
            "90_encoder.mvit.1.conv3.BatchNorm2d_1                          [128]   \n",
            "91_encoder.mvit.1.conv3.ReLU_2                                     -   \n",
            "92_encoder.mvit.1.conv4.0.Conv2d_depthwise          [256, 128, 3, 3]   \n",
            "93_encoder.mvit.1.conv4.0.Conv2d_pointwise          [128, 128, 1, 1]   \n",
            "94_encoder.mvit.1.conv4.BatchNorm2d_1                          [128]   \n",
            "95_encoder.mvit.1.conv4.ReLU_2                                     -   \n",
            "96_encoder.mv2.6.conv.Conv2d_0                      [128, 512, 1, 1]   \n",
            "97_encoder.mv2.6.conv.BatchNorm2d_1                            [512]   \n",
            "98_encoder.mv2.6.conv.ReLU_2                                       -   \n",
            "99_encoder.mv2.6.conv.Conv2d_3                        [1, 512, 3, 3]   \n",
            "100_encoder.mv2.6.conv.BatchNorm2d_4                           [512]   \n",
            "101_encoder.mv2.6.conv.ReLU_5                                      -   \n",
            "102_encoder.mv2.6.conv.Conv2d_6                     [512, 160, 1, 1]   \n",
            "103_encoder.mv2.6.conv.BatchNorm2d_7                           [160]   \n",
            "104_encoder.mvit.2.conv1.0.Conv2d_depthwise         [160, 160, 3, 3]   \n",
            "105_encoder.mvit.2.conv1.0.Conv2d_pointwise         [160, 160, 1, 1]   \n",
            "106_encoder.mvit.2.conv1.BatchNorm2d_1                         [160]   \n",
            "107_encoder.mvit.2.conv1.ReLU_2                                    -   \n",
            "108_encoder.mvit.2.conv2.Conv2d_0                   [160, 240, 1, 1]   \n",
            "109_encoder.mvit.2.conv2.BatchNorm2d_1                         [240]   \n",
            "110_encoder.mvit.2.conv2.ReLU_2                                    -   \n",
            "111_encoder.mvit.2.transformer.layers.0.0.ModLa...             [240]   \n",
            "112_encoder.mvit.2.transformer.layers.0.0.fn.Av...                 -   \n",
            "113_encoder.mvit.2.transformer.layers.0.1.ModLa...             [240]   \n",
            "114_encoder.mvit.2.transformer.layers.0.1.fn.Co...  [240, 960, 1, 1]   \n",
            "115_encoder.mvit.2.transformer.layers.0.1.fn.Re...                 -   \n",
            "116_encoder.mvit.2.transformer.layers.0.1.fn.Dr...                 -   \n",
            "117_encoder.mvit.2.transformer.layers.0.1.fn.Co...  [960, 240, 1, 1]   \n",
            "118_encoder.mvit.2.transformer.layers.0.1.fn.Dr...                 -   \n",
            "119_encoder.mvit.2.conv3.Conv2d_0                   [240, 160, 1, 1]   \n",
            "120_encoder.mvit.2.conv3.BatchNorm2d_1                         [160]   \n",
            "121_encoder.mvit.2.conv3.ReLU_2                                    -   \n",
            "122_encoder.mvit.2.conv4.0.Conv2d_depthwise         [320, 160, 3, 3]   \n",
            "123_encoder.mvit.2.conv4.0.Conv2d_pointwise         [160, 160, 1, 1]   \n",
            "124_encoder.mvit.2.conv4.BatchNorm2d_1                         [160]   \n",
            "125_encoder.mvit.2.conv4.ReLU_2                                    -   \n",
            "126_encoder.conv2.Conv2d_0                          [160, 320, 1, 1]   \n",
            "127_encoder.conv2.BatchNorm2d_1                                [320]   \n",
            "128_encoder.conv2.ReLU_2                                           -   \n",
            "129_decoder.Conv2d_conv2d_in                        [320, 128, 1, 1]   \n",
            "130_decoder.ups_block_1.ConvTranspose2d_conv2d_...   [64, 128, 3, 3]   \n",
            "131_decoder.ups_block_1.end_up_layer.0.Conv2d_d...   [192, 64, 3, 3]   \n",
            "132_decoder.ups_block_1.end_up_layer.0.Conv2d_p...    [64, 64, 1, 1]   \n",
            "133_decoder.ups_block_1.end_up_layer.ReLU_1                        -   \n",
            "134_decoder.ups_block_2.ConvTranspose2d_conv2d_...    [32, 64, 3, 3]   \n",
            "135_decoder.ups_block_2.end_up_layer.0.Conv2d_d...   [128, 32, 3, 3]   \n",
            "136_decoder.ups_block_2.end_up_layer.0.Conv2d_p...    [32, 32, 1, 1]   \n",
            "137_decoder.ups_block_2.end_up_layer.ReLU_1                        -   \n",
            "138_decoder.ups_block_3.ConvTranspose2d_conv2d_...    [16, 32, 3, 3]   \n",
            "139_decoder.ups_block_3.end_up_layer.0.Conv2d_d...    [80, 16, 3, 3]   \n",
            "140_decoder.ups_block_3.end_up_layer.0.Conv2d_p...    [16, 16, 1, 1]   \n",
            "141_decoder.ups_block_3.end_up_layer.ReLU_1                        -   \n",
            "142_decoder.Conv2d_conv2d_out                          [16, 1, 3, 3]   \n",
            "\n",
            "                                                         Output Shape  \\\n",
            "Layer                                                                   \n",
            "0_encoder.conv1.0.Conv2d_depthwise                   [1, 16, 96, 128]   \n",
            "1_encoder.conv1.0.Conv2d_pointwise                   [1, 16, 96, 128]   \n",
            "2_encoder.conv1.BatchNorm2d_1                        [1, 16, 96, 128]   \n",
            "3_encoder.conv1.ReLU_2                               [1, 16, 96, 128]   \n",
            "4_encoder.mv2.0.conv.Conv2d_0                        [1, 64, 96, 128]   \n",
            "5_encoder.mv2.0.conv.BatchNorm2d_1                   [1, 64, 96, 128]   \n",
            "6_encoder.mv2.0.conv.ReLU_2                          [1, 64, 96, 128]   \n",
            "7_encoder.mv2.0.conv.Conv2d_3                        [1, 64, 96, 128]   \n",
            "8_encoder.mv2.0.conv.BatchNorm2d_4                   [1, 64, 96, 128]   \n",
            "9_encoder.mv2.0.conv.ReLU_5                          [1, 64, 96, 128]   \n",
            "10_encoder.mv2.0.conv.Conv2d_6                       [1, 32, 96, 128]   \n",
            "11_encoder.mv2.0.conv.BatchNorm2d_7                  [1, 32, 96, 128]   \n",
            "12_encoder.mv2.1.conv.Conv2d_0                      [1, 128, 96, 128]   \n",
            "13_encoder.mv2.1.conv.BatchNorm2d_1                 [1, 128, 96, 128]   \n",
            "14_encoder.mv2.1.conv.ReLU_2                        [1, 128, 96, 128]   \n",
            "15_encoder.mv2.1.conv.Conv2d_3                       [1, 128, 48, 64]   \n",
            "16_encoder.mv2.1.conv.BatchNorm2d_4                  [1, 128, 48, 64]   \n",
            "17_encoder.mv2.1.conv.ReLU_5                         [1, 128, 48, 64]   \n",
            "18_encoder.mv2.1.conv.Conv2d_6                        [1, 64, 48, 64]   \n",
            "19_encoder.mv2.1.conv.BatchNorm2d_7                   [1, 64, 48, 64]   \n",
            "20_encoder.mv2.2.conv.Conv2d_0                       [1, 256, 48, 64]   \n",
            "21_encoder.mv2.2.conv.BatchNorm2d_1                  [1, 256, 48, 64]   \n",
            "22_encoder.mv2.2.conv.ReLU_2                         [1, 256, 48, 64]   \n",
            "23_encoder.mv2.2.conv.Conv2d_3                       [1, 256, 48, 64]   \n",
            "24_encoder.mv2.2.conv.BatchNorm2d_4                  [1, 256, 48, 64]   \n",
            "25_encoder.mv2.2.conv.ReLU_5                         [1, 256, 48, 64]   \n",
            "26_encoder.mv2.2.conv.Conv2d_6                        [1, 64, 48, 64]   \n",
            "27_encoder.mv2.2.conv.BatchNorm2d_7                   [1, 64, 48, 64]   \n",
            "28_encoder.mv2.3.conv.Conv2d_0                       [1, 256, 48, 64]   \n",
            "29_encoder.mv2.3.conv.BatchNorm2d_1                  [1, 256, 48, 64]   \n",
            "30_encoder.mv2.3.conv.ReLU_2                         [1, 256, 48, 64]   \n",
            "31_encoder.mv2.3.conv.Conv2d_3                       [1, 256, 48, 64]   \n",
            "32_encoder.mv2.3.conv.BatchNorm2d_4                  [1, 256, 48, 64]   \n",
            "33_encoder.mv2.3.conv.ReLU_5                         [1, 256, 48, 64]   \n",
            "34_encoder.mv2.3.conv.Conv2d_6                        [1, 64, 48, 64]   \n",
            "35_encoder.mv2.3.conv.BatchNorm2d_7                   [1, 64, 48, 64]   \n",
            "36_encoder.mv2.4.conv.Conv2d_0                       [1, 256, 48, 64]   \n",
            "37_encoder.mv2.4.conv.BatchNorm2d_1                  [1, 256, 48, 64]   \n",
            "38_encoder.mv2.4.conv.ReLU_2                         [1, 256, 48, 64]   \n",
            "39_encoder.mv2.4.conv.Conv2d_3                       [1, 256, 24, 32]   \n",
            "40_encoder.mv2.4.conv.BatchNorm2d_4                  [1, 256, 24, 32]   \n",
            "41_encoder.mv2.4.conv.ReLU_5                         [1, 256, 24, 32]   \n",
            "42_encoder.mv2.4.conv.Conv2d_6                        [1, 96, 24, 32]   \n",
            "43_encoder.mv2.4.conv.BatchNorm2d_7                   [1, 96, 24, 32]   \n",
            "44_encoder.mvit.0.conv1.0.Conv2d_depthwise            [1, 96, 24, 32]   \n",
            "45_encoder.mvit.0.conv1.0.Conv2d_pointwise            [1, 96, 24, 32]   \n",
            "46_encoder.mvit.0.conv1.BatchNorm2d_1                 [1, 96, 24, 32]   \n",
            "47_encoder.mvit.0.conv1.ReLU_2                        [1, 96, 24, 32]   \n",
            "48_encoder.mvit.0.conv2.Conv2d_0                     [1, 144, 24, 32]   \n",
            "49_encoder.mvit.0.conv2.BatchNorm2d_1                [1, 144, 24, 32]   \n",
            "50_encoder.mvit.0.conv2.ReLU_2                       [1, 144, 24, 32]   \n",
            "51_encoder.mvit.0.transformer.layers.0.0.ModLay...   [1, 144, 24, 32]   \n",
            "52_encoder.mvit.0.transformer.layers.0.0.fn.Avg...   [1, 144, 24, 32]   \n",
            "53_encoder.mvit.0.transformer.layers.0.1.ModLay...   [1, 144, 24, 32]   \n",
            "54_encoder.mvit.0.transformer.layers.0.1.fn.Con...   [1, 288, 24, 32]   \n",
            "55_encoder.mvit.0.transformer.layers.0.1.fn.ReL...   [1, 288, 24, 32]   \n",
            "56_encoder.mvit.0.transformer.layers.0.1.fn.Dro...   [1, 288, 24, 32]   \n",
            "57_encoder.mvit.0.transformer.layers.0.1.fn.Con...   [1, 144, 24, 32]   \n",
            "58_encoder.mvit.0.transformer.layers.0.1.fn.Dro...   [1, 144, 24, 32]   \n",
            "59_encoder.mvit.0.conv3.Conv2d_0                      [1, 96, 24, 32]   \n",
            "60_encoder.mvit.0.conv3.BatchNorm2d_1                 [1, 96, 24, 32]   \n",
            "61_encoder.mvit.0.conv3.ReLU_2                        [1, 96, 24, 32]   \n",
            "62_encoder.mvit.0.conv4.0.Conv2d_depthwise            [1, 96, 24, 32]   \n",
            "63_encoder.mvit.0.conv4.0.Conv2d_pointwise            [1, 96, 24, 32]   \n",
            "64_encoder.mvit.0.conv4.BatchNorm2d_1                 [1, 96, 24, 32]   \n",
            "65_encoder.mvit.0.conv4.ReLU_2                        [1, 96, 24, 32]   \n",
            "66_encoder.mv2.5.conv.Conv2d_0                       [1, 384, 24, 32]   \n",
            "67_encoder.mv2.5.conv.BatchNorm2d_1                  [1, 384, 24, 32]   \n",
            "68_encoder.mv2.5.conv.ReLU_2                         [1, 384, 24, 32]   \n",
            "69_encoder.mv2.5.conv.Conv2d_3                       [1, 384, 12, 16]   \n",
            "70_encoder.mv2.5.conv.BatchNorm2d_4                  [1, 384, 12, 16]   \n",
            "71_encoder.mv2.5.conv.ReLU_5                         [1, 384, 12, 16]   \n",
            "72_encoder.mv2.5.conv.Conv2d_6                       [1, 128, 12, 16]   \n",
            "73_encoder.mv2.5.conv.BatchNorm2d_7                  [1, 128, 12, 16]   \n",
            "74_encoder.mvit.1.conv1.0.Conv2d_depthwise           [1, 128, 12, 16]   \n",
            "75_encoder.mvit.1.conv1.0.Conv2d_pointwise           [1, 128, 12, 16]   \n",
            "76_encoder.mvit.1.conv1.BatchNorm2d_1                [1, 128, 12, 16]   \n",
            "77_encoder.mvit.1.conv1.ReLU_2                       [1, 128, 12, 16]   \n",
            "78_encoder.mvit.1.conv2.Conv2d_0                     [1, 192, 12, 16]   \n",
            "79_encoder.mvit.1.conv2.BatchNorm2d_1                [1, 192, 12, 16]   \n",
            "80_encoder.mvit.1.conv2.ReLU_2                       [1, 192, 12, 16]   \n",
            "81_encoder.mvit.1.transformer.layers.0.0.ModLay...   [1, 192, 12, 16]   \n",
            "82_encoder.mvit.1.transformer.layers.0.0.fn.Avg...   [1, 192, 12, 16]   \n",
            "83_encoder.mvit.1.transformer.layers.0.1.ModLay...   [1, 192, 12, 16]   \n",
            "84_encoder.mvit.1.transformer.layers.0.1.fn.Con...   [1, 768, 12, 16]   \n",
            "85_encoder.mvit.1.transformer.layers.0.1.fn.ReL...   [1, 768, 12, 16]   \n",
            "86_encoder.mvit.1.transformer.layers.0.1.fn.Dro...   [1, 768, 12, 16]   \n",
            "87_encoder.mvit.1.transformer.layers.0.1.fn.Con...   [1, 192, 12, 16]   \n",
            "88_encoder.mvit.1.transformer.layers.0.1.fn.Dro...   [1, 192, 12, 16]   \n",
            "89_encoder.mvit.1.conv3.Conv2d_0                     [1, 128, 12, 16]   \n",
            "90_encoder.mvit.1.conv3.BatchNorm2d_1                [1, 128, 12, 16]   \n",
            "91_encoder.mvit.1.conv3.ReLU_2                       [1, 128, 12, 16]   \n",
            "92_encoder.mvit.1.conv4.0.Conv2d_depthwise           [1, 128, 12, 16]   \n",
            "93_encoder.mvit.1.conv4.0.Conv2d_pointwise           [1, 128, 12, 16]   \n",
            "94_encoder.mvit.1.conv4.BatchNorm2d_1                [1, 128, 12, 16]   \n",
            "95_encoder.mvit.1.conv4.ReLU_2                       [1, 128, 12, 16]   \n",
            "96_encoder.mv2.6.conv.Conv2d_0                       [1, 512, 12, 16]   \n",
            "97_encoder.mv2.6.conv.BatchNorm2d_1                  [1, 512, 12, 16]   \n",
            "98_encoder.mv2.6.conv.ReLU_2                         [1, 512, 12, 16]   \n",
            "99_encoder.mv2.6.conv.Conv2d_3                         [1, 512, 6, 8]   \n",
            "100_encoder.mv2.6.conv.BatchNorm2d_4                   [1, 512, 6, 8]   \n",
            "101_encoder.mv2.6.conv.ReLU_5                          [1, 512, 6, 8]   \n",
            "102_encoder.mv2.6.conv.Conv2d_6                        [1, 160, 6, 8]   \n",
            "103_encoder.mv2.6.conv.BatchNorm2d_7                   [1, 160, 6, 8]   \n",
            "104_encoder.mvit.2.conv1.0.Conv2d_depthwise            [1, 160, 6, 8]   \n",
            "105_encoder.mvit.2.conv1.0.Conv2d_pointwise            [1, 160, 6, 8]   \n",
            "106_encoder.mvit.2.conv1.BatchNorm2d_1                 [1, 160, 6, 8]   \n",
            "107_encoder.mvit.2.conv1.ReLU_2                        [1, 160, 6, 8]   \n",
            "108_encoder.mvit.2.conv2.Conv2d_0                      [1, 240, 6, 8]   \n",
            "109_encoder.mvit.2.conv2.BatchNorm2d_1                 [1, 240, 6, 8]   \n",
            "110_encoder.mvit.2.conv2.ReLU_2                        [1, 240, 6, 8]   \n",
            "111_encoder.mvit.2.transformer.layers.0.0.ModLa...     [1, 240, 6, 8]   \n",
            "112_encoder.mvit.2.transformer.layers.0.0.fn.Av...     [1, 240, 6, 8]   \n",
            "113_encoder.mvit.2.transformer.layers.0.1.ModLa...     [1, 240, 6, 8]   \n",
            "114_encoder.mvit.2.transformer.layers.0.1.fn.Co...     [1, 960, 6, 8]   \n",
            "115_encoder.mvit.2.transformer.layers.0.1.fn.Re...     [1, 960, 6, 8]   \n",
            "116_encoder.mvit.2.transformer.layers.0.1.fn.Dr...     [1, 960, 6, 8]   \n",
            "117_encoder.mvit.2.transformer.layers.0.1.fn.Co...     [1, 240, 6, 8]   \n",
            "118_encoder.mvit.2.transformer.layers.0.1.fn.Dr...     [1, 240, 6, 8]   \n",
            "119_encoder.mvit.2.conv3.Conv2d_0                      [1, 160, 6, 8]   \n",
            "120_encoder.mvit.2.conv3.BatchNorm2d_1                 [1, 160, 6, 8]   \n",
            "121_encoder.mvit.2.conv3.ReLU_2                        [1, 160, 6, 8]   \n",
            "122_encoder.mvit.2.conv4.0.Conv2d_depthwise            [1, 160, 6, 8]   \n",
            "123_encoder.mvit.2.conv4.0.Conv2d_pointwise            [1, 160, 6, 8]   \n",
            "124_encoder.mvit.2.conv4.BatchNorm2d_1                 [1, 160, 6, 8]   \n",
            "125_encoder.mvit.2.conv4.ReLU_2                        [1, 160, 6, 8]   \n",
            "126_encoder.conv2.Conv2d_0                             [1, 320, 6, 8]   \n",
            "127_encoder.conv2.BatchNorm2d_1                        [1, 320, 6, 8]   \n",
            "128_encoder.conv2.ReLU_2                               [1, 320, 6, 8]   \n",
            "129_decoder.Conv2d_conv2d_in                           [1, 128, 6, 8]   \n",
            "130_decoder.ups_block_1.ConvTranspose2d_conv2d_...    [1, 64, 12, 16]   \n",
            "131_decoder.ups_block_1.end_up_layer.0.Conv2d_d...    [1, 64, 12, 16]   \n",
            "132_decoder.ups_block_1.end_up_layer.0.Conv2d_p...    [1, 64, 12, 16]   \n",
            "133_decoder.ups_block_1.end_up_layer.ReLU_1           [1, 64, 12, 16]   \n",
            "134_decoder.ups_block_2.ConvTranspose2d_conv2d_...    [1, 32, 24, 32]   \n",
            "135_decoder.ups_block_2.end_up_layer.0.Conv2d_d...    [1, 32, 24, 32]   \n",
            "136_decoder.ups_block_2.end_up_layer.0.Conv2d_p...    [1, 32, 24, 32]   \n",
            "137_decoder.ups_block_2.end_up_layer.ReLU_1           [1, 32, 24, 32]   \n",
            "138_decoder.ups_block_3.ConvTranspose2d_conv2d_...    [1, 16, 48, 64]   \n",
            "139_decoder.ups_block_3.end_up_layer.0.Conv2d_d...    [1, 16, 48, 64]   \n",
            "140_decoder.ups_block_3.end_up_layer.0.Conv2d_p...    [1, 16, 48, 64]   \n",
            "141_decoder.ups_block_3.end_up_layer.ReLU_1           [1, 16, 48, 64]   \n",
            "142_decoder.Conv2d_conv2d_out                          [1, 1, 48, 64]   \n",
            "\n",
            "                                                      Params    Mult-Adds  \n",
            "Layer                                                                      \n",
            "0_encoder.conv1.0.Conv2d_depthwise                     432.0    5.308416M  \n",
            "1_encoder.conv1.0.Conv2d_pointwise                     256.0    3.145728M  \n",
            "2_encoder.conv1.BatchNorm2d_1                           32.0         16.0  \n",
            "3_encoder.conv1.ReLU_2                                     -            -  \n",
            "4_encoder.mv2.0.conv.Conv2d_0                         1.024k   12.582912M  \n",
            "5_encoder.mv2.0.conv.BatchNorm2d_1                     128.0         64.0  \n",
            "6_encoder.mv2.0.conv.ReLU_2                                -            -  \n",
            "7_encoder.mv2.0.conv.Conv2d_3                          576.0    7.077888M  \n",
            "8_encoder.mv2.0.conv.BatchNorm2d_4                     128.0         64.0  \n",
            "9_encoder.mv2.0.conv.ReLU_5                                -            -  \n",
            "10_encoder.mv2.0.conv.Conv2d_6                        2.048k   25.165824M  \n",
            "11_encoder.mv2.0.conv.BatchNorm2d_7                     64.0         32.0  \n",
            "12_encoder.mv2.1.conv.Conv2d_0                        4.096k   50.331648M  \n",
            "13_encoder.mv2.1.conv.BatchNorm2d_1                    256.0        128.0  \n",
            "14_encoder.mv2.1.conv.ReLU_2                               -            -  \n",
            "15_encoder.mv2.1.conv.Conv2d_3                        1.152k    3.538944M  \n",
            "16_encoder.mv2.1.conv.BatchNorm2d_4                    256.0        128.0  \n",
            "17_encoder.mv2.1.conv.ReLU_5                               -            -  \n",
            "18_encoder.mv2.1.conv.Conv2d_6                        8.192k   25.165824M  \n",
            "19_encoder.mv2.1.conv.BatchNorm2d_7                    128.0         64.0  \n",
            "20_encoder.mv2.2.conv.Conv2d_0                       16.384k   50.331648M  \n",
            "21_encoder.mv2.2.conv.BatchNorm2d_1                    512.0        256.0  \n",
            "22_encoder.mv2.2.conv.ReLU_2                               -            -  \n",
            "23_encoder.mv2.2.conv.Conv2d_3                        2.304k    7.077888M  \n",
            "24_encoder.mv2.2.conv.BatchNorm2d_4                    512.0        256.0  \n",
            "25_encoder.mv2.2.conv.ReLU_5                               -            -  \n",
            "26_encoder.mv2.2.conv.Conv2d_6                       16.384k   50.331648M  \n",
            "27_encoder.mv2.2.conv.BatchNorm2d_7                    128.0         64.0  \n",
            "28_encoder.mv2.3.conv.Conv2d_0                       16.384k   50.331648M  \n",
            "29_encoder.mv2.3.conv.BatchNorm2d_1                    512.0        256.0  \n",
            "30_encoder.mv2.3.conv.ReLU_2                               -            -  \n",
            "31_encoder.mv2.3.conv.Conv2d_3                        2.304k    7.077888M  \n",
            "32_encoder.mv2.3.conv.BatchNorm2d_4                    512.0        256.0  \n",
            "33_encoder.mv2.3.conv.ReLU_5                               -            -  \n",
            "34_encoder.mv2.3.conv.Conv2d_6                       16.384k   50.331648M  \n",
            "35_encoder.mv2.3.conv.BatchNorm2d_7                    128.0         64.0  \n",
            "36_encoder.mv2.4.conv.Conv2d_0                       16.384k   50.331648M  \n",
            "37_encoder.mv2.4.conv.BatchNorm2d_1                    512.0        256.0  \n",
            "38_encoder.mv2.4.conv.ReLU_2                               -            -  \n",
            "39_encoder.mv2.4.conv.Conv2d_3                        2.304k    1.769472M  \n",
            "40_encoder.mv2.4.conv.BatchNorm2d_4                    512.0        256.0  \n",
            "41_encoder.mv2.4.conv.ReLU_5                               -            -  \n",
            "42_encoder.mv2.4.conv.Conv2d_6                       24.576k   18.874368M  \n",
            "43_encoder.mv2.4.conv.BatchNorm2d_7                    192.0         96.0  \n",
            "44_encoder.mvit.0.conv1.0.Conv2d_depthwise           82.944k   63.700992M  \n",
            "45_encoder.mvit.0.conv1.0.Conv2d_pointwise            9.216k    7.077888M  \n",
            "46_encoder.mvit.0.conv1.BatchNorm2d_1                  192.0         96.0  \n",
            "47_encoder.mvit.0.conv1.ReLU_2                             -            -  \n",
            "48_encoder.mvit.0.conv2.Conv2d_0                     13.824k   10.616832M  \n",
            "49_encoder.mvit.0.conv2.BatchNorm2d_1                  288.0        144.0  \n",
            "50_encoder.mvit.0.conv2.ReLU_2                             -            -  \n",
            "51_encoder.mvit.0.transformer.layers.0.0.ModLay...     288.0        144.0  \n",
            "52_encoder.mvit.0.transformer.layers.0.0.fn.Avg...         -            -  \n",
            "53_encoder.mvit.0.transformer.layers.0.1.ModLay...     288.0        144.0  \n",
            "54_encoder.mvit.0.transformer.layers.0.1.fn.Con...    41.76k   31.850496M  \n",
            "55_encoder.mvit.0.transformer.layers.0.1.fn.ReL...         -            -  \n",
            "56_encoder.mvit.0.transformer.layers.0.1.fn.Dro...         -            -  \n",
            "57_encoder.mvit.0.transformer.layers.0.1.fn.Con...   41.616k   31.850496M  \n",
            "58_encoder.mvit.0.transformer.layers.0.1.fn.Dro...         -            -  \n",
            "59_encoder.mvit.0.conv3.Conv2d_0                     13.824k   10.616832M  \n",
            "60_encoder.mvit.0.conv3.BatchNorm2d_1                  192.0         96.0  \n",
            "61_encoder.mvit.0.conv3.ReLU_2                             -            -  \n",
            "62_encoder.mvit.0.conv4.0.Conv2d_depthwise          165.888k  127.401984M  \n",
            "63_encoder.mvit.0.conv4.0.Conv2d_pointwise            9.216k    7.077888M  \n",
            "64_encoder.mvit.0.conv4.BatchNorm2d_1                  192.0         96.0  \n",
            "65_encoder.mvit.0.conv4.ReLU_2                             -            -  \n",
            "66_encoder.mv2.5.conv.Conv2d_0                       36.864k   28.311552M  \n",
            "67_encoder.mv2.5.conv.BatchNorm2d_1                    768.0        384.0  \n",
            "68_encoder.mv2.5.conv.ReLU_2                               -            -  \n",
            "69_encoder.mv2.5.conv.Conv2d_3                        3.456k     663.552k  \n",
            "70_encoder.mv2.5.conv.BatchNorm2d_4                    768.0        384.0  \n",
            "71_encoder.mv2.5.conv.ReLU_5                               -            -  \n",
            "72_encoder.mv2.5.conv.Conv2d_6                       49.152k    9.437184M  \n",
            "73_encoder.mv2.5.conv.BatchNorm2d_7                    256.0        128.0  \n",
            "74_encoder.mvit.1.conv1.0.Conv2d_depthwise          147.456k   28.311552M  \n",
            "75_encoder.mvit.1.conv1.0.Conv2d_pointwise           16.384k    3.145728M  \n",
            "76_encoder.mvit.1.conv1.BatchNorm2d_1                  256.0        128.0  \n",
            "77_encoder.mvit.1.conv1.ReLU_2                             -            -  \n",
            "78_encoder.mvit.1.conv2.Conv2d_0                     24.576k    4.718592M  \n",
            "79_encoder.mvit.1.conv2.BatchNorm2d_1                  384.0        192.0  \n",
            "80_encoder.mvit.1.conv2.ReLU_2                             -            -  \n",
            "81_encoder.mvit.1.transformer.layers.0.0.ModLay...     384.0        192.0  \n",
            "82_encoder.mvit.1.transformer.layers.0.0.fn.Avg...         -            -  \n",
            "83_encoder.mvit.1.transformer.layers.0.1.ModLay...     384.0        192.0  \n",
            "84_encoder.mvit.1.transformer.layers.0.1.fn.Con...  148.224k   28.311552M  \n",
            "85_encoder.mvit.1.transformer.layers.0.1.fn.ReL...         -            -  \n",
            "86_encoder.mvit.1.transformer.layers.0.1.fn.Dro...         -            -  \n",
            "87_encoder.mvit.1.transformer.layers.0.1.fn.Con...  147.648k   28.311552M  \n",
            "88_encoder.mvit.1.transformer.layers.0.1.fn.Dro...         -            -  \n",
            "89_encoder.mvit.1.conv3.Conv2d_0                     24.576k    4.718592M  \n",
            "90_encoder.mvit.1.conv3.BatchNorm2d_1                  256.0        128.0  \n",
            "91_encoder.mvit.1.conv3.ReLU_2                             -            -  \n",
            "92_encoder.mvit.1.conv4.0.Conv2d_depthwise          294.912k   56.623104M  \n",
            "93_encoder.mvit.1.conv4.0.Conv2d_pointwise           16.384k    3.145728M  \n",
            "94_encoder.mvit.1.conv4.BatchNorm2d_1                  256.0        128.0  \n",
            "95_encoder.mvit.1.conv4.ReLU_2                             -            -  \n",
            "96_encoder.mv2.6.conv.Conv2d_0                       65.536k   12.582912M  \n",
            "97_encoder.mv2.6.conv.BatchNorm2d_1                   1.024k        512.0  \n",
            "98_encoder.mv2.6.conv.ReLU_2                               -            -  \n",
            "99_encoder.mv2.6.conv.Conv2d_3                        4.608k     221.184k  \n",
            "100_encoder.mv2.6.conv.BatchNorm2d_4                  1.024k        512.0  \n",
            "101_encoder.mv2.6.conv.ReLU_5                              -            -  \n",
            "102_encoder.mv2.6.conv.Conv2d_6                       81.92k     3.93216M  \n",
            "103_encoder.mv2.6.conv.BatchNorm2d_7                   320.0        160.0  \n",
            "104_encoder.mvit.2.conv1.0.Conv2d_depthwise           230.4k     11.0592M  \n",
            "105_encoder.mvit.2.conv1.0.Conv2d_pointwise            25.6k      1.2288M  \n",
            "106_encoder.mvit.2.conv1.BatchNorm2d_1                 320.0        160.0  \n",
            "107_encoder.mvit.2.conv1.ReLU_2                            -            -  \n",
            "108_encoder.mvit.2.conv2.Conv2d_0                      38.4k      1.8432M  \n",
            "109_encoder.mvit.2.conv2.BatchNorm2d_1                 480.0        240.0  \n",
            "110_encoder.mvit.2.conv2.ReLU_2                            -            -  \n",
            "111_encoder.mvit.2.transformer.layers.0.0.ModLa...     480.0        240.0  \n",
            "112_encoder.mvit.2.transformer.layers.0.0.fn.Av...         -            -  \n",
            "113_encoder.mvit.2.transformer.layers.0.1.ModLa...     480.0        240.0  \n",
            "114_encoder.mvit.2.transformer.layers.0.1.fn.Co...   231.36k     11.0592M  \n",
            "115_encoder.mvit.2.transformer.layers.0.1.fn.Re...         -            -  \n",
            "116_encoder.mvit.2.transformer.layers.0.1.fn.Dr...         -            -  \n",
            "117_encoder.mvit.2.transformer.layers.0.1.fn.Co...   230.64k     11.0592M  \n",
            "118_encoder.mvit.2.transformer.layers.0.1.fn.Dr...         -            -  \n",
            "119_encoder.mvit.2.conv3.Conv2d_0                      38.4k      1.8432M  \n",
            "120_encoder.mvit.2.conv3.BatchNorm2d_1                 320.0        160.0  \n",
            "121_encoder.mvit.2.conv3.ReLU_2                            -            -  \n",
            "122_encoder.mvit.2.conv4.0.Conv2d_depthwise           460.8k     22.1184M  \n",
            "123_encoder.mvit.2.conv4.0.Conv2d_pointwise            25.6k      1.2288M  \n",
            "124_encoder.mvit.2.conv4.BatchNorm2d_1                 320.0        160.0  \n",
            "125_encoder.mvit.2.conv4.ReLU_2                            -            -  \n",
            "126_encoder.conv2.Conv2d_0                             51.2k      2.4576M  \n",
            "127_encoder.conv2.BatchNorm2d_1                        640.0        320.0  \n",
            "128_encoder.conv2.ReLU_2                                   -            -  \n",
            "129_decoder.Conv2d_conv2d_in                          40.96k     1.96608M  \n",
            "130_decoder.ups_block_1.ConvTranspose2d_conv2d_...   73.728k   14.155776M  \n",
            "131_decoder.ups_block_1.end_up_layer.0.Conv2d_d...  110.592k   21.233664M  \n",
            "132_decoder.ups_block_1.end_up_layer.0.Conv2d_p...    4.096k     786.432k  \n",
            "133_decoder.ups_block_1.end_up_layer.ReLU_1                -            -  \n",
            "134_decoder.ups_block_2.ConvTranspose2d_conv2d_...   18.432k   14.155776M  \n",
            "135_decoder.ups_block_2.end_up_layer.0.Conv2d_d...   36.864k   28.311552M  \n",
            "136_decoder.ups_block_2.end_up_layer.0.Conv2d_p...    1.024k     786.432k  \n",
            "137_decoder.ups_block_2.end_up_layer.ReLU_1                -            -  \n",
            "138_decoder.ups_block_3.ConvTranspose2d_conv2d_...    4.608k   14.155776M  \n",
            "139_decoder.ups_block_3.end_up_layer.0.Conv2d_d...    11.52k    35.38944M  \n",
            "140_decoder.ups_block_3.end_up_layer.0.Conv2d_p...     256.0     786.432k  \n",
            "141_decoder.ups_block_3.end_up_layer.ReLU_1                -            -  \n",
            "142_decoder.Conv2d_conv2d_out                          144.0     442.368k  \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "                            Totals\n",
            "Total params             3.220864M\n",
            "Trainable params         3.220864M\n",
            "Non-trainable params           0.0\n",
            "Mult-Adds             1.117478256G\n",
            "=======================================================================================================================\n",
            "The build_model model has: 3220864 trainable parameters\n",
            "Start training: build_model\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/37 [00:04<?, ?step/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ae44bf5dd008>\u001b[0m in \u001b[0;36m<cell line: 211>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Run process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ae44bf5dd008>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mrunning_l_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_l_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_l_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_l_ssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_DataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mtepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{global_var['epochs']} - Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def process(device):\n",
        "    # Set-seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "    np.random.seed(global_var['seed'])\n",
        "    torch.cuda.manual_seed(global_var['seed'])\n",
        "    # Datasets loading\n",
        "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
        "        dts_type=global_var['dts_type'],\n",
        "        dts_root_path=dataset_root,\n",
        "        rgb_h_res=global_var['RGB_img_res'][1],\n",
        "        d_h_res=global_var['D_img_res'][1],\n",
        "        bs_train=global_var['batch_size'],\n",
        "        bs_eval=global_var['batch_size_eval'],\n",
        "        num_workers=global_var['n_workers'],\n",
        "        size_train=global_var['size_train'],\n",
        "        size_test=global_var['size_test']\n",
        "    )\n",
        "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
        "    # Prints samples\n",
        "    if global_var['do_prints']:\n",
        "        print(' --- Test samples --- ')\n",
        "        print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
        "                  save_model_root=save_model_root)\n",
        "        print(' --- Training augmented samples --- ')\n",
        "        print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
        "                  save_model_root=save_model_root)\n",
        "    if global_var['do_train']:\n",
        "        torch.cuda.empty_cache()\n",
        "        # Globals\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
        "                   'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
        "        min_rmse = float('inf')\n",
        "        min_acc = 0\n",
        "        train_loss_list = []\n",
        "        test_loss_list = []\n",
        "        # Loss\n",
        "        criterion = balanced_loss_function(device=device)\n",
        "        # Model\n",
        "        model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
        "        if global_var['do_pretrained'] or global_var['imagenet_w_init']:\n",
        "            model, _ = load_pretrained_model(model=model,\n",
        "                                             path_weigths=save_model_root + 'build_model_best' if global_var['do_pretrained']\n",
        "                                                          else imagenet_init + global_var['imagenet_enc'] + '/build_model_best',\n",
        "                                             device=device,\n",
        "                                             do_pretrained=global_var['do_pretrained'],\n",
        "                                             imagenet_w_init=global_var['imagenet_w_init'])\n",
        "        model_name = model.__class__.__name__\n",
        "        if global_var['do_print_model']:\n",
        "            print_model(model=model, device=device, save_model_root=save_model_root, input_shape=global_var['RGB_img_res'])\n",
        "        print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "        # Optimizer\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(), lr=global_var['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
        "        )\n",
        "        # Scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.1, patience=global_var['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
        "            cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
        "        )\n",
        "        # Early stopping\n",
        "        trigger_times, early_stopping_epochs = 0, global_var['e_stop_epochs']\n",
        "        print(\"Start training: {}\\n\".format(model_name))\n",
        "        # Train\n",
        "        for epoch in range(global_var['epochs']):\n",
        "            iter = 1\n",
        "            model.train()\n",
        "            running_loss, accuracy = 0, 0\n",
        "            running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
        "            with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "                for batch in tepoch:\n",
        "                    tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Training\")\n",
        "                    # Load data\n",
        "                    inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
        "                    # Forward\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    # Compute loss\n",
        "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
        "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
        "                    # Backward\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    # Evaluation and Stats\n",
        "                    running_loss += loss.item()\n",
        "                    running_l_mae += loss_depth.item()\n",
        "                    running_l_norm += loss_normal.item()\n",
        "                    running_l_grad += loss_grad.item()\n",
        "                    running_l_ssim += loss_ssim.item()\n",
        "\n",
        "                    train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
        "                    train_loss_list.append(train_loss_support)\n",
        "\n",
        "                    accuracy += compute_accuracy(outputs, depths)\n",
        "                    tepoch.set_postfix({'Loss': running_loss / iter,\n",
        "                                        'Acc': accuracy.item() / iter,\n",
        "                                        'Lr': global_var['lr'] if not history['lrs'] else history['lrs'][-1],\n",
        "                                        'L_mae': running_l_mae / iter,\n",
        "                                        'L_norm': running_l_norm / iter,\n",
        "                                        'L_grad': running_l_grad / iter,\n",
        "                                        'L_ssim': running_l_ssim / iter\n",
        "                                        })\n",
        "                    iter += 1\n",
        "\n",
        "            # Validation\n",
        "            iter = 1\n",
        "            model.eval()\n",
        "            test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
        "            with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "                for batch in tepoch:\n",
        "                    tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Validation\")\n",
        "                    inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
        "                    # Validation loop\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(inputs)\n",
        "                        # Evaluation metrics\n",
        "                        test_accuracy += compute_accuracy(outputs, depths)\n",
        "                        # Loss\n",
        "                        loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
        "                        loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
        "                        test_loss += loss.item()\n",
        "\n",
        "                        test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
        "                        test_loss_list.append(test_loss_support)\n",
        "\n",
        "                        # RMSE\n",
        "                        test_rmse += compute_rmse(outputs, depths)\n",
        "                        tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
        "                                            'RMSE': test_rmse.item() / iter})\n",
        "                        iter += 1\n",
        "\n",
        "            # Update history infos\n",
        "            history['lrs'].append(get_lr(optimizer))\n",
        "            history['train_loss'].append(running_loss / len(training_DataLoader))\n",
        "            history['val_loss'].append(test_loss / len(test_DataLoader))\n",
        "            history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
        "            history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
        "            history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
        "            # Update history losses infos\n",
        "            history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
        "            history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
        "            history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
        "            history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
        "            # Update scheduler LR\n",
        "            scheduler.step(history['test_rmse'][-1])\n",
        "            # Save model by best RMSE\n",
        "            if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
        "                trigger_times = 0\n",
        "                min_rmse = test_rmse / len(test_DataLoader)\n",
        "                save_checkpoint(model, model_name + '_best', save_model_root)\n",
        "                print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
        "            else:\n",
        "                trigger_times += 1\n",
        "                print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
        "            # Save model by best ACCURACY\n",
        "            if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
        "                min_acc = test_accuracy / len(test_DataLoader)\n",
        "                save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
        "                print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
        "                if trigger_times > 4:\n",
        "                    trigger_times = trigger_times - 2\n",
        "                    print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
        "\n",
        "            save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
        "                                     save_path=save_model_root + 'evolution_img/')\n",
        "            save_history(history, save_model_root + model_name + '_history')\n",
        "            # Empty CUDA cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if trigger_times == early_stopping_epochs:\n",
        "                print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
        "                break\n",
        "\n",
        "            # Save loss for graphs\n",
        "            np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
        "            np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
        "\n",
        "        print('Finished Training')\n",
        "        save_csv_history(model_name=model_name, path=save_model_root)\n",
        "        plot_history(history, path=save_model_root)\n",
        "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
        "\n",
        "        if global_var['do_prints']:\n",
        "            if os.path.exists(save_model_root + 'example&augment_img/'):\n",
        "                shutil.rmtree(save_model_root + 'example&augment_img/')\n",
        "\n",
        "    else:\n",
        "        model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
        "        model, model_name = load_pretrained_model(model=model,\n",
        "                                                  path_weigths=save_model_root + 'build_model_best',\n",
        "                                                  device=device,\n",
        "                                                  do_pretrained=global_var['do_pretrained'],\n",
        "                                                  imagenet_w_init=global_var['imagenet_w_init'])\n",
        "        if global_var['do_print_model']:\n",
        "            print_model(model=model, device=device, save_model_root=save_model_root,\n",
        "                        input_shape=global_var['RGB_img_res'])\n",
        "        print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "    # Evaluate\n",
        "    print(' --- Begin evaluation --- ')\n",
        "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
        "    print(' --- End evaluation --- ')\n",
        "\n",
        "    if global_var['do_print_best_worst']:\n",
        "        sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
        "        save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
        "        save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Hardware\n",
        "    device = hardware_check()\n",
        "\n",
        "    # -- TRAIN 1\n",
        "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
        "    # Directory test\n",
        "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
        "    #print(save_model_root)\n",
        "    # Create folders\n",
        "    if global_var['do_train']:\n",
        "        if not os.path.exists(save_model_root):\n",
        "            os.makedirs(save_model_root)\n",
        "        # if not os.path.exists(save_model_root + 'info_code/'):\n",
        "        #     os.makedirs(save_model_root + 'info_code/')\n",
        "        # files_directory = '/work/project/'\n",
        "        # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
        "        # for f in files:\n",
        "        #     shutil.copy(f, save_model_root + 'info_code/')\n",
        "    # Run process\n",
        "    start_time = perf_counter()\n",
        "    process(device=device)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = perf_counter()\n",
        "    print(\"Total time elapsed: \",end_time - start_time)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y17y55RlM8ZK"
      },
      "source": [
        "# Time Tests"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns4BuU6tsskL"
      },
      "source": [
        "## Inference time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "S2Aoo0hxNAxD",
        "outputId": "13d3bcd5-609c-448e-b502-fbbce5a0d75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment runned throught Colab CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-fee5c2f325c3>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Datasets loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mdts_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dts_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mdts_root_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bcb26bca0bf8>\u001b[0m in \u001b[0;36minit_train_test_loader\u001b[0;34m(dts_type, dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train, size_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     test_Dataset = Dataset_class(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdts_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdts_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb_h_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgb_h_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_h_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_h_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdts_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-9-fac1bbded78c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size, scenarios)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdepth_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdts_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/eigen_test_depth.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    431\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "meter_types = ['s','xs','xxs']\n",
        "test_rounds = 1\n",
        "blockPrint()\n",
        "device = hardware_check()\n",
        "enablePrint()\n",
        "\n",
        "if device == \"cpu\":\n",
        "  dev = !lscpu |grep 'Model name'\n",
        "  dev = str(dev).strip(\"]'\").split(\":\")[1].strip()\n",
        "  print(\"Experiment runned throught Colab CPU:\", dev)\n",
        "else:\n",
        "  dev = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  print(\"Experiment runned throught Colab GPU:\", dev[1])\n",
        "\n",
        "# Set-seed\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "np.random.seed(global_var['seed'])\n",
        "torch.cuda.manual_seed(global_var['seed'])\n",
        "\n",
        "# Datasets loading\n",
        "training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
        "      dts_type=global_var['dts_type'],\n",
        "      dts_root_path=dataset_root,\n",
        "      rgb_h_res=global_var['RGB_img_res'][1],\n",
        "      d_h_res=global_var['D_img_res'][1],\n",
        "      bs_train=global_var['batch_size'],\n",
        "      bs_eval=global_var['batch_size_eval'],\n",
        "      num_workers=global_var['n_workers'],\n",
        "      size_train=global_var['size_train'],\n",
        "      size_test=global_var['size_test']\n",
        "    )\n",
        "\n",
        "for arch_type in meter_types:\n",
        "  print(\"################################################################ TEST - %s architecture ################################################################\" % arch_type.upper())\n",
        "  times = np.ndarray(shape=test_rounds,dtype='float')\n",
        "\n",
        "  blockPrint()\n",
        "  model = build_model(device=device, arch_type=arch_type).to(device=device)\n",
        "  model, model_name = load_pretrained_model(model=model,\n",
        "                                            path_weigths=save_model_root + arch_type + '_build_model_best',\n",
        "                                            device=device,\n",
        "                                            do_pretrained=global_var['do_pretrained'],\n",
        "                                            imagenet_w_init=global_var['imagenet_w_init'])\n",
        "  enablePrint()\n",
        "\n",
        "\n",
        "  print(\"Model: %s\" % network_type)\n",
        "  blockPrint()\n",
        "  infos = summary(model,torch.ones(1,3,192,256).to(device))\n",
        "  enablePrint()\n",
        "  warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "  print(\"Trainable parameters: %d\" % count_parameters(model))\n",
        "  print(\"Mult-Adds: %d\" % int(infos[\"Mult-Adds\"].sum()))\n",
        "\n",
        "  for tests in range(test_rounds):\n",
        "      # Evaluate\n",
        "      blockPrint()\n",
        "      start_time = perf_counter()\n",
        "      best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
        "      #torch.cuda.synchronize() # <--------------------------------- REMEMBER WITH GPU\n",
        "      end_time = perf_counter()\n",
        "      enablePrint()\n",
        "\n",
        "      times[tests] = end_time - start_time\n",
        "\n",
        "  print(\"Average test time: \",np.mean(times))\n",
        "  print(\"Test times: \",times)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZYZ4AOswLY"
      },
      "source": [
        "## Transformer times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGWR2OLLszeV",
        "outputId": "1e3ad5b9-592d-49c2-d7a3-e63321e78f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment runned throught Colab CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "################################################################ TEST - S architecture ################################################################\n",
            "Model: MetaMETER\n",
            "Trainable parameters: 3220864\n",
            "Mult-Adds: 1117478256\n",
            "Mvit block 1 mean time:  0.0035114656595438467\n",
            "Mvit block 2 mean time:  0.0024989062061080447\n",
            "Mvit block 3 mean time:  0.0013980904396876975\n",
            "################################################################ TEST - XS architecture ################################################################\n",
            "Model: MetaMETER\n",
            "Trainable parameters: 1403320\n",
            "Mult-Adds: 637792248\n",
            "Mvit block 1 mean time:  0.0023549324442551514\n",
            "Mvit block 2 mean time:  0.001562212665682727\n",
            "Mvit block 3 mean time:  0.0008826994320326276\n",
            "################################################################ TEST - XXS architecture ################################################################\n",
            "Model: MetaMETER\n",
            "Trainable parameters: 682504\n",
            "Mult-Adds: 211970616\n",
            "Mvit block 1 mean time:  0.0022189052625609146\n",
            "Mvit block 2 mean time:  0.0010991420534020378\n",
            "Mvit block 3 mean time:  0.0005336074229087219\n"
          ]
        }
      ],
      "source": [
        "meter_types = ['s','xs','xxs']\n",
        "test_rounds = 30\n",
        "blockPrint()\n",
        "device = hardware_check()\n",
        "enablePrint()\n",
        "\n",
        "if device == \"cpu\":\n",
        "  dev = !lscpu |grep 'Model name'\n",
        "  dev = str(dev).strip(\"]'\").split(\":\")[1].strip()\n",
        "  print(\"Experiment runned throught Colab CPU:\", dev)\n",
        "else:\n",
        "  dev = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  print(\"Experiment runned throught Colab GPU:\", dev[1])\n",
        "\n",
        "# Set-seed\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "np.random.seed(global_var['seed'])\n",
        "torch.cuda.manual_seed(global_var['seed'])\n",
        "\n",
        "# Datasets loading\n",
        "training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
        "      dts_type=global_var['dts_type'],\n",
        "      dts_root_path=dataset_root,\n",
        "      rgb_h_res=global_var['RGB_img_res'][1],\n",
        "      d_h_res=global_var['D_img_res'][1],\n",
        "      bs_train=global_var['batch_size'],\n",
        "      bs_eval=global_var['batch_size_eval'],\n",
        "      num_workers=global_var['n_workers'],\n",
        "      size_train=global_var['size_train'],\n",
        "      size_test=global_var['size_test']\n",
        "    )\n",
        "\n",
        "for arch_type in meter_types:\n",
        "  print(\"################################################################ TEST - %s architecture ################################################################\" % arch_type.upper())\n",
        "  times = np.ndarray(shape=(test_rounds,3,655),dtype='float')\n",
        "\n",
        "  blockPrint()\n",
        "  model = build_model(device=device, arch_type=arch_type).to(device=device)\n",
        "  model, model_name = load_pretrained_model(model=model,\n",
        "                                            path_weigths=save_model_root + arch_type + '_build_model_best',\n",
        "                                            device=device,\n",
        "                                            do_pretrained=global_var['do_pretrained'],\n",
        "                                            imagenet_w_init=global_var['imagenet_w_init'])\n",
        "  enablePrint()\n",
        "\n",
        "\n",
        "  print(\"Model: %s\" % network_type)\n",
        "  blockPrint()\n",
        "  infos = summary(model,torch.ones(1,3,192,256).to(device))\n",
        "  enablePrint()\n",
        "  warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "  print(\"Trainable parameters: %d\" % count_parameters(model))\n",
        "  print(\"Mult-Adds: %d\" % int(infos[\"Mult-Adds\"].sum()))\n",
        "\n",
        "  for tests in range(test_rounds):\n",
        "      # Evaluate\n",
        "      blockPrint()\n",
        "      best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
        "      enablePrint()\n",
        "      times[tests][0] = model.transformer_times[0]\n",
        "      times[tests][1] = model.transformer_times[1]\n",
        "      times[tests][2] = model.transformer_times[2]\n",
        "\n",
        "  print(\"Mvit block 1 mean time: \",np.mean(times[tests][0]))\n",
        "  print(\"Mvit block 2 mean time: \",np.mean(times[tests][1]))\n",
        "  print(\"Mvit block 3 mean time: \",np.mean(times[tests][2]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ff-Jl-vO8S"
      },
      "source": [
        "# Debug (too genius can break things)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvJ82XLlvS_p",
        "outputId": "7eee7490-5e32-4e4e-a972-d813db09aecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug\n"
          ]
        }
      ],
      "source": [
        "enablePrint()\n",
        "print(\"Debug\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "g2bPw-5W4hJe",
        "Nm2TAq6B5UBI",
        "BoQYEe4V5j5E",
        "bbCnAwv453IN",
        "tAWQQQzf8ctn",
        "eAvmPzvA8Pu-",
        "EjiqGK4q42zP",
        "Ns4BuU6tsskL"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
